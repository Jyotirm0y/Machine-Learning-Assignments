{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "# # Standardizing the data.\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.08 seconds.\n",
      "Convergence after 10 epochs took 0.08 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
       "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
       "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
       " (1, 15),\n",
       " array([-0.8531383]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83336"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    \n",
    "    w = np.zeros_like(dim) # initializing weight as array of zeros using numpy\n",
    "    b = 0 # initializing bias as zero\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57349184 -0.19015688 -0.06584143 -0.86990562 -2.80927706 -1.43345052\n",
      "  0.35862361  0.24627836 -2.25803168 -0.87761289  2.31023199 -0.3484947\n",
      " -2.2575668  -1.93628665  1.65242231]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "print(dim)\n",
    "w,b = initialize_weights(dim)\n",
    "print(w)\n",
    "def grader_weights(w,b):\n",
    "    assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "    return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig = 1/(1+np.exp(-z))\n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "    val=sigmoid(z)\n",
    "    assert(val==0.8807970779778823)\n",
    "    return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    temp = 0\n",
    "    loss = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(len(y_true)):\n",
    "#         print(y_true[i])\n",
    "#         print(y_pred[i])\n",
    "        temp = temp + y_true[i]*np.log10(y_pred[i]) + (1-y_true[i])*np.log10(1-y_pred[i])\n",
    "    \n",
    "    loss = -1* (temp/n)\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "    loss=logloss(true,pred)\n",
    "#     print(loss)\n",
    "#     print(type(loss))\n",
    "    assert(loss==0.07644900402910389)\n",
    "    return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    z = np.dot(w.T,x) + b\n",
    "    sigma = 1 / (1 + np.exp(-z))\n",
    "    dw = x*(y - sigma) - ((alpha/N)*w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.03932417 -1.65802126  0.39552179  1.93522773  0.57391643  1.40717219\n",
      "  0.43385535  0.02036643 -0.42413939 -0.99725862 -1.83576236 -0.00725938\n",
      " -1.00531444 -0.03686952  2.77293046]\n",
      "2.613689585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "    grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "    print(grad_dw)\n",
    "    print(np.sum(grad_dw))\n",
    "    assert(np.sum(grad_dw)==2.613689585)\n",
    "    return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    z = np.dot(w.T,x) + b\n",
    "    sigma = 1/(1+ np.exp(-z))\n",
    "    db = y-sigma\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "    grad_db=gradient_db(x,y,w,b)\n",
    "    print(grad_db)\n",
    "    assert(grad_db==-0.5)\n",
    "    return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "\n",
    "    \n",
    "    # initializing weight and bias\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    \n",
    "    # No.of data points\n",
    "    N = len(X_train)\n",
    "    \n",
    "    # lists to store losses for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for e in range(epochs): # for each epoch\n",
    "        \n",
    "        for i in range(len(X_train)): # for each datapoint\n",
    "            \n",
    "            # find gradient of weight and biases\n",
    "            dw = gradient_dw(X_train[i], y_train[i], w, b, alpha, N) \n",
    "            db = gradient_db(X_train[i], y_train[i], w, b)\n",
    "\n",
    "            # update weight and biases\n",
    "            w = w + eta0*dw\n",
    "            b = b + eta0*db\n",
    "\n",
    "        ###############################################################################\n",
    "        ##### calculate train loss for total data points using updataed w and b #######\n",
    "        y_train_pred = []\n",
    "        for i in range(len(X_train)):\n",
    "            x = X_train[i]\n",
    "            z = np.dot(w.T,x) + b\n",
    "            y_predicted = sigmoid(z)\n",
    "            y_train_pred.append(y_predicted)\n",
    "\n",
    "        train_loss = logloss(y_train, y_train_pred)\n",
    "        train_losses.append(train_loss)\n",
    "        ###############################################################################\n",
    "\n",
    "        ###############################################################################\n",
    "        ### Calculate test loss for total data points using updated w and b ###########\n",
    "        y_test_pred = []  \n",
    "        for i in range(len(X_test)):\n",
    "            x = X_test[i]\n",
    "            z = np.dot(w.T,x) + b\n",
    "            y_predicted = sigmoid(z)\n",
    "            y_test_pred.append(y_predicted)\n",
    "\n",
    "        test_loss = logloss(y_test, y_test_pred)\n",
    "        test_losses.append(test_loss)\n",
    "        ###############################################################################\n",
    "        \n",
    "        ########################### print details of each epoch #######################\n",
    "        print(\"epoch :\", e+1, \" Train loss: \",train_loss, \" Test loss: \",test_loss)\n",
    "        \n",
    "        ###### Check, if there is no minimization in test loss then return ############\n",
    "#         if e!=0 and (test_losses[e]-test_losses[e-1]) < 0.00001: # if not first epoch\n",
    "#                 return w, b\n",
    "        \n",
    "    return w,b,train_losses,test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1  Train loss:  0.17545748442854608  Test loss:  0.1759547442321374\n",
      "epoch : 2  Train loss:  0.16867157050333045  Test loss:  0.16939931358951013\n",
      "epoch : 3  Train loss:  0.1663916799246292  Test loss:  0.16720591194885742\n",
      "epoch : 4  Train loss:  0.16536827537403162  Test loss:  0.1662171779933495\n",
      "epoch : 5  Train loss:  0.16485707459547083  Test loss:  0.16571959463978406\n",
      "epoch : 6  Train loss:  0.1645882001292827  Test loss:  0.1654555709550864\n",
      "epoch : 7  Train loss:  0.16444271323364382  Test loss:  0.1653113502079951\n",
      "epoch : 8  Train loss:  0.16436263615826985  Test loss:  0.1652311685317927\n",
      "epoch : 9  Train loss:  0.16431806946667746  Test loss:  0.1651860589844903\n",
      "epoch : 10  Train loss:  0.1642930737413251  Test loss:  0.16516045651849884\n",
      "epoch : 11  Train loss:  0.1642789743093407  Test loss:  0.16514582028704106\n",
      "epoch : 12  Train loss:  0.16427098545835503  Test loss:  0.16513739835366367\n",
      "epoch : 13  Train loss:  0.1642664419100352  Test loss:  0.16513252084404828\n",
      "epoch : 14  Train loss:  0.16426384911424854  Test loss:  0.1651296765934633\n",
      "epoch : 15  Train loss:  0.16426236468266475  Test loss:  0.1651280051869749\n",
      "epoch : 16  Train loss:  0.16426151190514926  Test loss:  0.16512701421034165\n",
      "epoch : 17  Train loss:  0.16426102013167446  Test loss:  0.16512642050180534\n",
      "epoch : 18  Train loss:  0.1642607352750557  Test loss:  0.16512606043934003\n",
      "epoch : 19  Train loss:  0.1642605693884229  Test loss:  0.16512583897905464\n",
      "epoch : 20  Train loss:  0.16426047215122602  Test loss:  0.16512570058201864\n",
      "epoch : 21  Train loss:  0.16426041469677802  Test loss:  0.16512561256556316\n",
      "epoch : 22  Train loss:  0.1642603804172862  Test loss:  0.16512555553536112\n",
      "epoch : 23  Train loss:  0.16426035972510467  Test loss:  0.16512551786674262\n",
      "epoch : 24  Train loss:  0.1642603470622927  Test loss:  0.1651254925088858\n",
      "epoch : 25  Train loss:  0.16426033919038657  Test loss:  0.1651254751256673\n",
      "epoch : 26  Train loss:  0.1642603342104248  Test loss:  0.16512546300819161\n",
      "epoch : 27  Train loss:  0.1642603310001301  Test loss:  0.16512545443448257\n",
      "epoch : 28  Train loss:  0.16426032888986594  Test loss:  0.16512544828939998\n",
      "epoch : 29  Train loss:  0.16426032747541439  Test loss:  0.16512544383683098\n",
      "epoch : 30  Train loss:  0.16426032650945105  Test loss:  0.16512544058152992\n",
      "epoch : 31  Train loss:  0.16426032583825456  Test loss:  0.16512543818419045\n",
      "epoch : 32  Train loss:  0.16426032536459445  Test loss:  0.16512543640841265\n",
      "epoch : 33  Train loss:  0.16426032502581867  Test loss:  0.16512543508700558\n",
      "epoch : 34  Train loss:  0.16426032478075378  Test loss:  0.16512543410018002\n",
      "epoch : 35  Train loss:  0.16426032460180723  Test loss:  0.16512543336117225\n",
      "epoch : 36  Train loss:  0.16426032447014866  Test loss:  0.16512543280655914\n",
      "epoch : 37  Train loss:  0.1642603243726961  Test loss:  0.16512543238964958\n",
      "epoch : 38  Train loss:  0.16426032430021448  Test loss:  0.16512543207585434\n",
      "epoch : 39  Train loss:  0.164260324246105  Test loss:  0.16512543183944392\n",
      "epoch : 40  Train loss:  0.16426032420559727  Test loss:  0.1651254316612056\n",
      "epoch : 41  Train loss:  0.1642603241752011  Test loss:  0.165125431526748\n",
      "epoch : 42  Train loss:  0.16426032415235478  Test loss:  0.16512543142527641\n",
      "epoch : 43  Train loss:  0.16426032413516095  Test loss:  0.16512543134867216\n",
      "epoch : 44  Train loss:  0.16426032412221028  Test loss:  0.16512543129082813\n",
      "epoch : 45  Train loss:  0.16426032411244418  Test loss:  0.16512543124714216\n",
      "epoch : 46  Train loss:  0.16426032410507813  Test loss:  0.16512543121414358\n",
      "epoch : 47  Train loss:  0.16426032409951882  Test loss:  0.1651254311892149\n",
      "epoch : 48  Train loss:  0.1642603240953229  Test loss:  0.1651254311703816\n",
      "epoch : 49  Train loss:  0.16426032409215466  Test loss:  0.16512543115615208\n",
      "epoch : 50  Train loss:  0.16426032408976063  Test loss:  0.16512543114540162\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w_,b_, train_loss_, test_loss_=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:'Segoe UI';font-size:18px;color:blue\"><b>Plotting train and test loss</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch = range(epochs)\n",
    "plt.plot(epoch, train_loss_, 'r', label='Training loss')\n",
    "plt.plot(epoch, test_loss_, 'g', label='Test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;font-family:'Segoe UI';font-size:18px\"><b>Training with loss comparison</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "\n",
    "    \n",
    "    # initializing weight and bias\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    \n",
    "    # No.of data points\n",
    "    N = len(X_train)\n",
    "    \n",
    "    # lists to store losses for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    epoch_no=0\n",
    "    for e in range(epochs): # for each epoch\n",
    "        epoch_no+=1 #  update the epoch_no\n",
    "        for i in range(len(X_train)): # for each datapoint\n",
    "            \n",
    "            # find gradient of weight and biases\n",
    "            dw = gradient_dw(X_train[i], y_train[i], w, b, alpha, N) \n",
    "            db = gradient_db(X_train[i], y_train[i], w, b)\n",
    "\n",
    "            # update weight and biases\n",
    "            w = w + eta0*dw\n",
    "            b = b + eta0*db\n",
    "\n",
    "        ###############################################################################\n",
    "        ##### calculate train loss for total data points using updataed w and b #######\n",
    "        y_train_pred = []\n",
    "        for i in range(len(X_train)):\n",
    "            x = X_train[i]\n",
    "            z = np.dot(w.T,x) + b\n",
    "            y_predicted = sigmoid(z)\n",
    "            y_train_pred.append(y_predicted)\n",
    "\n",
    "        train_loss = logloss(y_train, y_train_pred)\n",
    "        train_losses.append(train_loss)\n",
    "        ###############################################################################\n",
    "\n",
    "        ###############################################################################\n",
    "        ### Calculate test loss for total data points using updated w and b ###########\n",
    "        y_test_pred = []  \n",
    "        for i in range(len(X_test)):\n",
    "            x = X_test[i]\n",
    "            z = np.dot(w.T,x) + b\n",
    "            y_predicted = sigmoid(z)\n",
    "            y_test_pred.append(y_predicted)\n",
    "\n",
    "        test_loss = logloss(y_test, y_test_pred)\n",
    "        test_losses.append(test_loss)\n",
    "        ###############################################################################\n",
    "        \n",
    "        ########################### print details of each epoch #######################\n",
    "        print(\"epoch :\", e+1, \" Train loss: \",train_loss, \" Test loss: \",test_loss)\n",
    "        \n",
    "        ###### Check, if there is no minimization in test loss and if loss start increasing then return ############\n",
    "        if e!=0 and (test_losses[e]-test_losses[e-1]) > 0.0 : # if not first epoch\n",
    "                return w, b,train_losses,test_losses,epoch_no\n",
    "        \n",
    "    return w,b,train_losses,test_losses,epoch_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1  Train loss:  0.17545748442854608  Test loss:  0.1759547442321374\n",
      "epoch : 2  Train loss:  0.16867157050333045  Test loss:  0.16939931358951013\n",
      "epoch : 3  Train loss:  0.1663916799246292  Test loss:  0.16720591194885742\n",
      "epoch : 4  Train loss:  0.16536827537403162  Test loss:  0.1662171779933495\n",
      "epoch : 5  Train loss:  0.16485707459547083  Test loss:  0.16571959463978406\n",
      "epoch : 6  Train loss:  0.1645882001292827  Test loss:  0.1654555709550864\n",
      "epoch : 7  Train loss:  0.16444271323364382  Test loss:  0.1653113502079951\n",
      "epoch : 8  Train loss:  0.16436263615826985  Test loss:  0.1652311685317927\n",
      "epoch : 9  Train loss:  0.16431806946667746  Test loss:  0.1651860589844903\n",
      "epoch : 10  Train loss:  0.1642930737413251  Test loss:  0.16516045651849884\n",
      "epoch : 11  Train loss:  0.1642789743093407  Test loss:  0.16514582028704106\n",
      "epoch : 12  Train loss:  0.16427098545835503  Test loss:  0.16513739835366367\n",
      "epoch : 13  Train loss:  0.1642664419100352  Test loss:  0.16513252084404828\n",
      "epoch : 14  Train loss:  0.16426384911424854  Test loss:  0.1651296765934633\n",
      "epoch : 15  Train loss:  0.16426236468266475  Test loss:  0.1651280051869749\n",
      "epoch : 16  Train loss:  0.16426151190514926  Test loss:  0.16512701421034165\n",
      "epoch : 17  Train loss:  0.16426102013167446  Test loss:  0.16512642050180534\n",
      "epoch : 18  Train loss:  0.1642607352750557  Test loss:  0.16512606043934003\n",
      "epoch : 19  Train loss:  0.1642605693884229  Test loss:  0.16512583897905464\n",
      "epoch : 20  Train loss:  0.16426047215122602  Test loss:  0.16512570058201864\n",
      "epoch : 21  Train loss:  0.16426041469677802  Test loss:  0.16512561256556316\n",
      "epoch : 22  Train loss:  0.1642603804172862  Test loss:  0.16512555553536112\n",
      "epoch : 23  Train loss:  0.16426035972510467  Test loss:  0.16512551786674262\n",
      "epoch : 24  Train loss:  0.1642603470622927  Test loss:  0.1651254925088858\n",
      "epoch : 25  Train loss:  0.16426033919038657  Test loss:  0.1651254751256673\n",
      "epoch : 26  Train loss:  0.1642603342104248  Test loss:  0.16512546300819161\n",
      "epoch : 27  Train loss:  0.1642603310001301  Test loss:  0.16512545443448257\n",
      "epoch : 28  Train loss:  0.16426032888986594  Test loss:  0.16512544828939998\n",
      "epoch : 29  Train loss:  0.16426032747541439  Test loss:  0.16512544383683098\n",
      "epoch : 30  Train loss:  0.16426032650945105  Test loss:  0.16512544058152992\n",
      "epoch : 31  Train loss:  0.16426032583825456  Test loss:  0.16512543818419045\n",
      "epoch : 32  Train loss:  0.16426032536459445  Test loss:  0.16512543640841265\n",
      "epoch : 33  Train loss:  0.16426032502581867  Test loss:  0.16512543508700558\n",
      "epoch : 34  Train loss:  0.16426032478075378  Test loss:  0.16512543410018002\n",
      "epoch : 35  Train loss:  0.16426032460180723  Test loss:  0.16512543336117225\n",
      "epoch : 36  Train loss:  0.16426032447014866  Test loss:  0.16512543280655914\n",
      "epoch : 37  Train loss:  0.1642603243726961  Test loss:  0.16512543238964958\n",
      "epoch : 38  Train loss:  0.16426032430021448  Test loss:  0.16512543207585434\n",
      "epoch : 39  Train loss:  0.164260324246105  Test loss:  0.16512543183944392\n",
      "epoch : 40  Train loss:  0.16426032420559727  Test loss:  0.1651254316612056\n",
      "epoch : 41  Train loss:  0.1642603241752011  Test loss:  0.165125431526748\n",
      "epoch : 42  Train loss:  0.16426032415235478  Test loss:  0.16512543142527641\n",
      "epoch : 43  Train loss:  0.16426032413516095  Test loss:  0.16512543134867216\n",
      "epoch : 44  Train loss:  0.16426032412221028  Test loss:  0.16512543129082813\n",
      "epoch : 45  Train loss:  0.16426032411244418  Test loss:  0.16512543124714216\n",
      "epoch : 46  Train loss:  0.16426032410507813  Test loss:  0.16512543121414358\n",
      "epoch : 47  Train loss:  0.16426032409951882  Test loss:  0.1651254311892149\n",
      "epoch : 48  Train loss:  0.1642603240953229  Test loss:  0.1651254311703816\n",
      "epoch : 49  Train loss:  0.16426032409215466  Test loss:  0.16512543115615208\n",
      "epoch : 50  Train loss:  0.16426032408976063  Test loss:  0.16512543114540162\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,train_loss,test_loss,e=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU1b348c83ISEQMmHfQcAqEBACIiAoRdGKO/XWVkRFbat4r1C1LqjtrdV6r63WqlVfXFRErRWXivoruEtEKyqIaGVRFoNEQNYQQiDr9/fHeSYOYTKZSebJhMz3/XrNa2ae5TznzPZ9zjnPnCOqijHGGBOtlERnwBhjzOHFAocxxpiYWOAwxhgTEwscxhhjYmKBwxhjTEwscBhjjImJBY4YicirIjI13tsmkojki8gpic4H+J8XEckRkWV+pR/F8U8UkS8TdfxQNT+fIvIHEdkhIlvDbDteRAoaN4eJc7h8dyG+nykRuVdEptW5oao2+xtQHHKrAvaHPJ+S6Pwl+gbkA6eEWf5qyOtUDpSFPJ9Vj+PcBvytPnmJY1n/AVzQWMfz+X3LA34Rp7R6ed+LzrWsHw8UNIEyzwX+0MA0LgXeT3RZmuIN6AZsAtIjbdciujh0eFPVNsHHIpKP+7K9VXM7EWmhqhWNmbemTFVPDz4Wkbm4H47fJC5HDSMi3YCTgClxSk8AUdWqeKSXYEcAO1V1W6Iz4icRSdhv3uHw+6KqW0RkDXAO8EKkDZPqRsgZJt5ZFHATsBV4CmgH/BPYDuz2HvcM2T8P7ywP78wFuMfb9mvg9Hpu2xdYDOwF3gIeopaz8yjzeAfwLy+9N4COIesvBjYCO4FbieKsmxpnesBZwAqgEPgAGBKy7ibgW+/YXwITgIm4Gks5rsbyWRTvT0vgPmCzd7sPaOmt6+iVuxDYBbwHpNR2fG/5JcBbIcd6ioNroDd6y0d7ZSoEPgPG13ht7/Re2/3ADwAF/hNY6x3zDuBIYAlQBDyHdwZHjTN3r7zXA58De4BngYy63mcvD5XAAS/vD3rLBwBveq/Jl8BPI7ynecAvgFO8slR5ac0Ns23NfA/09i8EVgLnhKw7A1jlvRbfAtfX9Z7VOJYAfwG2ea/J58Bg4AoOrvn+P2/7mcB673irgB+HpHWp9179xTvmP7zXrNJLozDSaxPld7c98DjuM7obeCnC70tKSH53ep+N9iFpPe9tuwf3ezAoite15nuTTy2fKW/9jcAWL7+/wH1+fxCy/lbg8Yi/B4n48U7kjUMDRwXwR9yPVCugA/AfQGsgy3sjX4rwgSoHfgmkAld5b4bUY9sl3gczHTgB94NTW+CIJo/rgaO9MuUBd3nrcnBfmHFeme/1XoOoAwcwHPelHuWVZar3urYE+uOqut29bfsAR3qPb6utTLW8P7cDHwKdgU64H/M7vHX/C8wC0rzbibgfnEjHvxt4qLbjec974L7QZ+C+5Kd6zzuFvLbfAIOAFt6xFXgFCHjLS4G3gX5ANu7LPjXCl/xjoDvuB2g1MC2G9/kXIc8zvbJf5uVtOLCDkB+fGmWv3r9mvsJsW73eK/M64Bbc5/Vk3I9Zf2/9FuBE73E7YHik9yzMsU4DPgHaeu/pQKBbzc9hyPbne69fCvAzYF/I9pfiPt/TvdekFVE0VRHbd3cB7se5nVeuH0b4fbkG95nu6S37P+CZkONe7r3XwZOmFSHrantdD3rviPyZmogLTINwn6unODRwnAcsj/T6WOe4O8v6naqWqup+Vd2pqv9Q1RJV3Ys7s/thhP03quojqloJPIFrI+wSy7Yi0hs4DvhvVS1T1fdxP0RhRZnHx1X1K1XdjzuryfWW/wT4p6ouVtVS4LfeaxCLXwL/p6ofqWqlqj6B+7EcjTuTawnkiEiaquar6voY0w+aAtyuqttUdTvwe1xtCdwXuRtwhKqWq+p76j71kY7fFvcDF8lFwEJVXaiqVar6JrAMF0iC5qrqSlWtUNVyb9kfVbVIVVcCXwBvqOoGVd2D6ysaFuGYD6jqZlXdBfw/vPeqHp/Fs4B8VX3cy9ty3Bn2T+ooc6xGA21wJyNlqvoOriYx2Vtfjnv9A6q628tHcHm496ymctyP5wDcj/NqVd1SW2ZU9Xnv9atS1WdxNb+RIZtsVtW/eq/J/nqWubbvbjfgdNwP826vXO+G7HfQ7wtwJXCrqhZ437/bgJ8Em9BUdY6q7g1ZN1REskNel3CvazhhP1PAT3G/DStVtQT3nappL+67UisLHLBdVQ8En4hIaxH5PxHZKCJFuOpiWxFJrWX/6itQvDcC3Jcqlm27A7tCloE7cwwryjyGXhlTEpKn7qFpq+o+3Bl1LI4Afi0ihcEbrnO1u6quw51V3QZsE5F5ItI9xvSDuuOa1II2esvA1R7WAW+IyAYRmemVJ9Lxd+N+kOoq2/k1ynYC7ociKNx7813I4/1hntf2mYBa3qt6fBaPAEbVyPsUoGuEY9dHd2CTHty3sxFXWwNXSzoD2Cgi74rI8d7ysO9ZTV4gehDXXPudiMwWkUBtmRGRS0RkRUiZB+OaxYJq/S7FoLbvbi/cd3d3Lfsd9PuCe4/mh+R1Ne5kp4uIpIrIXSKy3nu/8719gmWp7XWNmF8ifP8J/9pk4ZoTa2WBw1XTQv0a19wxSlUDuCYdcFVmv2wB2otI65BlvSJs35A8bglN2ztmh9iyyybgTlVtG3JrrarPAKjq31X1BNyXRHFVdTj0ta7LZi+NoN7eMryzsl+raj/gbOA6EZlQx/E/xzXfhaqZp03AUzXKlqmqd0XYxy91vc/h8v5ujby3UdWr4pyvzUAvEQn9/eiNa3dHVZeq6rm4JsaXcDXeiO9ZTar6gKoei2tSORq4IbgqdDsROQJ4BLga6KCqbXE1vtDvQs3XKZ7v3ybcd7e2M/Rw79HpNd6jDFX9FrgQOBfX55SNa2YFryy1va4x2oJrJgsK9zszENe3VysLHIfKwp0hFopIe+B3fh9QVTfimkNuE5F070zibJ/y+AJwloicICLpuH6EWD8HjwDTRGSUOJkicqaIZIlIfxE5WURa4joh9+POqMCdhfep8YMTyTPAb0Skk4h0BP4b+BuAiJwlIj/wrmwq8o5RWcfx3wSGi0hGyDG+w/VFBP0NOFtETvPOADO8/zCEftkaS13vc828/xM4WkQuFpE073aciAyMc74+wvUj3OgdYzzu8zrP+/xOEZFsrxkv+N7U+p7VTNzL8ygRSfOOc4CDP0OhZc7E/Thv9/a9DFfjiOQ7oKf3+W8QrwntVeBhEWnnvR7jIuwyC7jTC3h4n+1zvXVZuCbfnbj+h/8J7hTpdY3Rc8BlIjLQO2n87zDb/NArU60scBzqPlwn1g5cJ9ZrjXTcKcDxuA/NH3CdbaW1bFvvPHpt8P8F/B139rEbd+VH1FR1Ga6f40Fv/3W4DkRw/Qt3eXnbijs7usVb97x3v1NEIrXPBv0BF1A/B/4NLPeWARyFu/qsGHdhwcOqmhfp+Kr6HfAO7qwu6H9xwalQRK5X1U3e+ltwP0abcGe7ifiu1PU+349rH98tIg94/SA/Ai7A1Qq28n3HbNyoahnucs3Tvbw9DFyiqmu8TS4G8r3mlmm4fiOo/T2rKYA7OdnN91f/3eOtewzXzl8oIi+p6irgz1563wHH4K6iiuQd3JVgW0VkRwxFr83FuP6HNbiLRq6JsO39uP7LN0RkL+59HeWtexJX3m9xF1R8GOY44V7XqKnqq8ADwCLc93aJt6oUqi9Zz8HVaGoVvCrANDEi8iywRlV9r/EkExHJwXVujqylY9aYpOHVRr/AXeZeISJ/Btar6sMR97PvTtMgIsfhrjP/GnfW+BJwvKp+mtCMGWOaFRH5Me4S4kzcSVSVqk6KJQ1rqmo6uuKuHS/GVSWvsqBhjPHBlbhm2PW4fpKYL56wGocxxpiYWI3DGGNMTJJikMOOHTtqnz59Im6zb98+MjMzGydDTYiVO7lYuZNPQ8r+ySef7FDVTjWXJ0Xg6NOnD8uWRZ6CIS8vj/HjxzdOhpoQK3dysXInn4aUXUQ2hltuTVXGGGNiYoHDGGNMTCxwGGOMiUlS9HEYY5qu8vJyCgoKOHDgQN0b11N2djarV6/2Lf2mLJqyZ2Rk0LNnT9LS0qJK09fAISITcWOzpAKP1hhhFBEZgJs5azhujPp7vOX9cWM1BfXDzVVxn7d+Om40zApggare6Gc5jDH+KSgoICsriz59+uDGP4y/vXv3kpVV14j6zVNdZVdVdu7cSUFBAX379o0qTd8ChzdnwEO4GdQKgKUi8oo3KFnQLmAGcNDf3VX1S7yJR7x0vgXme89Pwg1CN0RVS0Wks19lMMb478CBA74GDROZiNChQwe2b98e9T5+9nGMBNapmwWtDJjHwaOSom5mt6W4kSVrMwE36FbwsrCrcDOPlQbTiH/WjTGNyYJGYsX6+vvZVNWDg2eXKuD74YNjcQFuXoago4ETReRO3Dj913vB5yAicgVucnu6dOlCXl5exIMUFxcfss2SnUv4et/XXNj7wnpk+/AQrtzJwMrddGRnZ7N3b10z+jZMZWWl78doqqIt+4EDB6L/bGiECckbcsNNIP9oyPOLgb/Wsu1tuABQc3k6brz/LiHLvsANAii4Ws3XhJnwPvR27LHHal0WLVp0yLLpC6dr27va1rnv4SxcuZOBlbvpWLVqle/HKCoqqnXdjh07dOjQoTp06FDt0qWLdu/evfp5aWlpxHSXLl2q06dPr/P4xx9/fMx5DmfRokV65plnxrRPpLKHCvc+AMs0zG+qnzWOAg6elrAn3rSfMTgdWK5uAp7QdF/0CvWxiFTh5uSNvoEuStktsykqLUJVrSptTDPVoUMHVqxYAcBtt91GmzZtuP7666vXV1RU0KJF+J/KESNGMGLEiDqP8cEHH8Qns02En30cS4GjRKSvN0XjBbiZr2IxmYObqcDNU3EygIgczfe1krgLtAxQpVWUlJfUvbExptm49NJLue666zjppJO46aab+PjjjxkzZgzDhg1jzJgxfPnll4AbzuOss84CXNC5/PLLGT9+PP369eOBBx6oTq9NmzbV248fP56f/OQnDBgwgClTpgRbUli4cCEDBgzghBNOYMaMGdXp1mbXrl1MmjSJIUOGMHr0aD7//HMA3n33XXJzc8nNzWXYsGHs3buXLVu2MG7cOHJzcxk8eDDvvfdeg14f32oc6maTuhp4HXc57hxVXSki07z1s0SkK25q0ABQJSLXADmqWuTNh3sqbuz4UHOAOSLyBVAGTNXgKx9ngZYBAPaU7iEzPTkHSDOmUV1zDXhn/3GTmwt33BHzbl999RVvvfUWqampFBUVsXjxYlq0aMFbb73FLbfcwj/+8Y9D9lmzZg2LFi1i79699O/fn6uuuuqQ/0Z8+umnrFy5ku7duzN27Fj+9a9/MWLECK688koWL15M3759mTx5cp35+93vfsewYcN46aWXeOedd7jkkktYsWIF99xzDw899BBjx46luLiY8vJy5syZw2mnncatt95KZWUlJSUNOxn29X8cqroQWFhj2ayQx1txTVjh9i0BOoRZXkY95tqtj+yMbACKSovontW9MQ5pjGkizj//fFJTUwHYs2cPU6dOZe3atYgI5eXhLwQ988wzadmyJS1btqRz585899139Ox58E/cyJEjq5fl5uaSn59PmzZt6NevX/X/KCZPnszs2bMj5u/999+vDl4nn3wyO3fuZM+ePYwdO5brrruOKVOmcN5555Gdnc1xxx3H5ZdfTnl5OZMmTSI3N7dBr439czyCYI2jqLQowTkxJkncd58/6dbjiqrQoch/+9vfctJJJzF//nzy8/NrHW22ZcuW1Y9TU1OpqKiIapv6NJqE20dEmDlzJmeeeSYLFy5k9OjRvPzyy4wbN47FixezYMECLr74Ym644QYuueSSmI8ZZGNVRVDdVHVgT4JzYoxJpD179tCjRw8A5s6dG/f0BwwYwIYNG8jPzwfg2WefjbwDMG7cOJ5++mnA9Z107NiRQCDA+vXrOeaYY7jpppsYMWIEX331FRs3bqRz58788pe/5Oc//znLly9vUH6txhFBdoV7eazGYUxyu/HGG5k6dSr33nsvJ598ctzTb9WqFQ8//DATJ06kY8eOjBw5ss59brvtNi677DKGDBlC69ateeKJJwC47777WLRoEampqeTk5HDqqaeyYMEC7r77btLS0mjTpg1PPvlkwzIc7hrd5nar7/848v/zQuU2dM7yOXXuf7hqitf1NwYrd9OR6P9xNBV79+5VVdWqqiq96qqr9N57741Lun78j8OaqiIIZLq++T2l1lRljPHXI488Qm5uLoMGDWLPnj1ceWXNC0qbDmuqiiCQ1RGqoKhkd6KzYoxp5q699lquvfbaRGcjKlbjiCA1uy2ZZbBnry//LzTGmMOSBY5IAgECpVC0b2eic2KMMU2GBY5IsrLIPmBNVcYYE8oCRyRejWPP/sJE58QYY5oM6xyPJNhUVZac4/gbkwx27tzJhAkTANi6dSupqal06tQJgI8//pj09PSI++fl5ZGens6YMWMOWTd37lyWLVvGgw8+GP+MJ5AFjkgCAbJLYYsFDmOarbqGVa9LXl4ebdq0CRs4mitrqook2FRVsS/ROTHGNKJPPvmEH/7whxx77LGcdtppbNmyBYAHHniAnJwchgwZwgUXXEB+fj6zZs3iL3/5C7m5uRGHK9+4cSMTJkxgyJAhTJgwgW+++QaA559/nsGDBzN06FDGjRsHwMqVKxk5ciS5ubkMGTKEtWvX+l/oGFiNI5JgU1XV/kTnxJikcM1r17Bia3yHVc/tmssdY6MfVl1VmT59Oi+//DKdOnXi2Wef5dZbb2XOnDncddddfP3117Rs2ZLCwkLatm3LtGnToqqlXH311VxyySVMnTqVOXPmMGPGDF566SVuv/12Xn/9dXr06EFhoetPnTVrFr/61a+YMmUKZWVlVFZWNug1iDcLHJFkZpJdCnv1AFVaRYpYBc2Y5q60tJQvvviCU089FXBzdnfr1g2AIUOGMGXKFCZNmsSkSZNiSnfJkiW8+OKLAFx88cXceOONAIwdO5ZLL72Un/70p5x33nkAHH/88dx5550UFBRw3nnncdRRR8WreHFhgSOSlBQC2hKVUorLiqtHyzXG+OO+if4Mq743hmHVVZVBgwaxZMmSQ9YtWLCAxYsX88orr3DHHXewcuXKeucpOB31rFmz+Oijj1iwYAG5ubmsWLGCCy+8kFGjRrFgwQJOO+00Hn30UV8GV6wvO4WuQ3ZKK8BGyDUmWbRs2ZLt27dXB47y8nJWrlxJVVUVmzZt4qSTTuJPf/oThYWFFBcXk5WVFVVgGjNmDPPmzQPg6aef5oQTTgBg/fr1jBo1ittvv52OHTuyadMmNmzYQL9+/ZgxYwbnnHNO9bSwTYUFjjoEWrjJXCxwGJMcUlJSeOGFF7jpppsYOnQoubm5fPDBB1RWVnLRRRdxzDHHMGzYMK699lratm3L2Wefzfz58+vsHH/ggQd4/PHHGTJkCE899RT3338/ADfccAPHHHMMgwcPZty4cQwdOpRnn32WwYMHk5uby5o1axo06ZIfrKmqDoE0N8m8TeZkTPN32223VT9evHjxIevff//9Q5YdffTRtdYILr30Ui699FIA+vTpwzvvvHPINsF+j1A333wzN998c5S5bnxW46hDtk0fa4wxB7HAUYdARlvAAocxxgRZ4KhDoLULHDaZkzH+cZPNmUSJ9fW3wFGH7NbtAatxGOOXjIwMdu7cacEjQVSVnTt3kpGREfU+1jlehzaBjgAUWee4Mb7o2bMnBQUFbN++3bdjHDhwIKYfxuYkmrJnZGTQs2fPqNO0wFGHlEA2WbthT7FN5mSMH9LS0ujbt6+vx8jLy2PYsGG+HqOp8qPsvjZVichEEflSRNaJyMww6weIyBIRKRWR60OW9xeRFSG3IhG5psa+14uIikhHP8tAIOAmc7JZAI0xBvCxxiEiqcBDwKlAAbBURF5R1VUhm+0CZgAHDfqiql8CuSHpfAvMD0m7l5fuN37lv1pwoEObBdAYYwB/axwjgXWqukFVy4B5wLmhG6jqNlVdCpRHSGcCsF5VN4Ys+wtwI+B/b5rNAmiMMQfxs4+jB7Ap5HkBMKoe6VwAPBN8IiLnAN+q6mfBQcLCEZErgCsAunTpQl5eXsSDFBcXh90me/16skth864tdaZxOKqt3M2dlTu5JGu5wZ+y+xk4wv2qx1RDEJF04BzgZu95a+BW4Ed17auqs4HZACNGjNDx48dH3D4vL4+w27RtS+B92JhWGX79Ya7WcjdzVu7kkqzlBn/K7mdTVQHQK+R5T2BzjGmcDixX1e+850cCfYHPRCTfS3O5iHRtYF5rF+zjsFkAjTEG8LfGsRQ4SkT64jq3LwAujDGNyYQ0U6nqv4HOwede8BihqjsanNvaBK+qqrRZAI0xBnwMHKpaISJXA68DqcAcVV0pItO89bO8msIyIABUeZfc5qhqkdcsdSpwpV95jEpWFoFSKKaUyqpKUlNSE5odY4xJNF//AKiqC4GFNZbNCnm8FdfcFG7fEqBDHen3aXgu69CyJdmVqUAle8v20tYb9NAYY5KVjVUVhYDNAmiMMdUscEQhkOpmAbTJnIwxxgJHVLJbuFkArcZhjDEWOKISaJkFWOAwxhiwwBGVQEY2YJM5GWMMWOCISnardoDVOIwxBixwRCXQxl0VbIHDGGMscEQlM6s9KVV2VZUxxoAFjqhIINuNV7Xf5uQwxhgLHNEIDnRYvCvROTHGmISzwBENb7yqPSU2fawxxljgiEYgQHYpFO23Pg5jjLHAEY1gU5V1jhtjjAWOqATnHS/fm+icGGNMwlngiEZwMqdymwXQGGMscEQj2FRVVZLonBhjTMJZ4IiGFzhKtIzyyvJE58YYYxLKAkc0MjPJLnUP95ZZP4cxJrlZ4IiGCAHJAGzYEWOMscARpWxvFkAb6NAYk+wscEQpkGaBwxhjwAJH1ALpAcAmczLGGAscUcr2ZgG0GocxJtlZ4IhSoLXNAmiMMWCBI2rBwGFXVRljkp2vgUNEJorIlyKyTkRmhlk/QESWiEipiFwfsry/iKwIuRWJyDXeurtFZI2IfC4i80WkrZ9lCGqV1Y4WlVbjMMYY3wKHiKQCDwGnAznAZBHJqbHZLmAGcE/oQlX9UlVzVTUXOBYoAeZ7q98EBqvqEOAr4Ga/yhCqehZA6xw3xiQ5P2scI4F1qrpBVcuAecC5oRuo6jZVXQpEGsdjArBeVTd6+7yhqhXeug+BnvHPehjBEXKLbTInY0xya+Fj2j2ATSHPC4BR9UjnAuCZWtZdDjwbboWIXAFcAdClSxfy8vIiHqS4uDjiNt22bCG7FPIL1teZ1uGkrnI3V1bu5JKs5QZ/yu5n4JAwyzSmBETSgXMI0xwlIrcCFcDT4fZV1dnAbIARI0bo+PHjIx4rLy+PiNts3UrgDWjRKiXydoeZOsvdTFm5k0uylhv8KbufgaMA6BXyvCewOcY0TgeWq+p3oQtFZCpwFjBBVWMKRvXmNVVtOVDYKIczxpimys8+jqXAUSLS16s5XAC8EmMak6nRTCUiE4GbgHNUtfEmyMjKcpM52ei4xpgk51uNQ1UrRORq4HUgFZijqitFZJq3fpaIdAWWAQGgyrvkNkdVi0SkNXAqcGWNpB8EWgJvigjAh6o6za9yVAtO5lRhswAaY5Kbn01VqOpCYGGNZbNCHm+llquivNpEhzDLfxDnbEYneFWVBQ5jTJKzf45HKxAguxRKqaC0ojTRuTHGmISxwBGtrCwCNgugMcZY4IhaejrZlWmAjVdljEluFjhiEEhtDdh4VcaY5GaBIwaBFjYLoDHGWOCIQXZ6FmCzABpjkpsFjhgEbBZAY4yxwBGLQCs39Yd1jhtjkpkFjhhkZ7YHrMZhjEluFjhi0DKrHek2C6AxJslZ4IhFcNgR6xw3xiQxCxyxCI6Qu393onNijDEJY4EjFsERcvdZ4DDGJC8LHLEINlWVWOAwxiQvCxyxCARcU5X1cRhjkpgFjlgEm6pKbXRcY0zyssARi+rJnIoTnRNjjEkYCxyx8CZzKqrYh6omOjfGGJMQUQUOEckUkRTv8dEico6IpPmbtSbIm8ypnEpKK20WQGNMcoq2xrEYyBCRHsDbwGXAXL8y1WR5TVVg41UZY5JXtIFDVLUEOA/4q6r+GMjxL1tNVGYm2V7gsGFHjDHJKurAISLHA1OABd6yFv5kqQkTsVkAjTFJL9rAcQ1wMzBfVVeKSD9gkX/ZarqyvVkAbbwqY0yyiqrWoKrvAu8CeJ3kO1R1hp8Za6oCaVnAdqtxGGOSVrRXVf1dRAIikgmsAr4UkRv8zVrTZLMAGmOSXbRNVTmqWgRMAhYCvYGL69pJRCaKyJcisk5EZoZZP0BElohIqYhcH7K8v4isCLkVicg13rr2IvKmiKz17ttFWYa4yPYCh11VZYxJVtEGjjTvfxuTgJdVtRyI+A84EUkFHgJOx12BNVlEal6JtQuYAdwTulBVv1TVXFXNBY4FSoD53uqZwNuqehTu0uBDApKfsmwWQGNMkos2cPwfkA9kAotF5Aigrl/OkcA6Vd2gqmXAPODc0A1UdZuqLgXKI6QzAVivqhu95+cCT3iPn8AFs0aTntWWjAoLHMaY5BVt5/gDwAMhizaKyEl17NYD2BTyvAAYFVv2ALgAeCbkeRdV3eLla4uIdA63k4hcAVwB0KVLF/Ly8iIepLi4uM5tAI4sKiK7Paz+enVU2zd10Za7ubFyJ5dkLTf4U/aoAoeIZAO/A8Z5i94FbgciNfRLmGUxDfAkIunAObhLgWOiqrOB2QAjRozQ8ePHR9w+Ly+PurYBYNEiAruhTfvM6LZv4qIudzNj5U4uyVpu8Kfs0TZVzQH2Aj/1bkXA43XsUwD0CnneE9gcY/5OB5ar6nchy74TkW4A3v22GNNsmOrJnHY16mGNMaapiDZwHKmqv/P6Kzao6u+BfnXssxQ4SkT6ejWHC4BXYszfZA5upsJLY6r3eCrwcoxpNkxwMqeSwkY9rPWxbTAAABd/SURBVDHGNBXRBo79InJC8ImIjAX2R9pBVSuAq4HXgdXAc96/zqeJyDQvna4iUgBcB/xGRApEJOCtaw2cCrxYI+m7gFNFZK23/q4oyxAf3gi5RXY5rjEmSUU73tQ04EmvrwNgN9+f9ddKVRfi/vcRumxWyOOtuCascPuWAB3CLN+Ju9IqMYJNVTbkiDEmSUV7VdVnwNBgbUBVg3/I+9zPzDVJIZM5GWNMMoppBkBVLfL+QQ6ueSn5BOcdt1kAjTFJqiFTx4a73Lb58wJHJVWUlJckOjfGGNPoGhI4kvN027uqCqyfwxiTnCL2cYjIXsIHCAFa+ZKjpi4ri27F7uGmPZvontU9sfkxxphGFrHGoapZqhoIc8tS1eSbARAgLY2cPekArNy+MsGZMcaYxteQpqqk1VezydBUVm6zwGGMST7JWWtooNSsbAaWVrBqx6pEZ8UYYxqd1TjqIxBgUHFrq3EYY5KSBY76CATIKUxjU9Emm5fDGJN0LHDURyDAoO3u4art1lxljEkuFjjqIxBg0JZKAGuuMsYkHQsc9ZGVRd/NJbRq0couyTXGJB0LHPURCJBStJeBnQZa4DDGJB0LHPURCEBZGYPaD7CmKmNM0rHAUR+BAACDsvrx7d5vKTxgswEaY5KHBY766ODmlxqU2g2wK6uMMcnFAkd99O8PQM4O99Saq4wxycQCR314gaPP2h20TmttNQ5jTFKxwFEfmZlwxBGkrPmSgR3tyipjTHKxwFFfAwfC6tUM6jzIAocxJqlY4KivnBxYs4ZBHXPYvHezXVlljEkaFjjqa+BA2L+fQdoRsA5yY0zysMBRXwMHAjBoh3sJrbnKGJMsLHDUlxc4eq/fQWZaptU4jDFJw9fAISITReRLEVknIjPDrB8gIktEpFRErq+xrq2IvCAia0RktYgc7y3PFZEPRWSFiCwTkZF+lqFW7dtDly6krF5DTqccq3EYY5KGb4FDRFKBh4DTgRxgsojk1NhsFzADuCdMEvcDr6nqAGAosNpb/ifg96qaC/y39zwxvCurLHAYY5KJnzWOkcA6Vd2gqmXAPODc0A1UdZuqLgXKQ5eLSAAYBzzmbVemqsHLlhQIeI+zgc3+FaEOwUtyO+WwtXgru/bvSlhWjDGmsbTwMe0ewKaQ5wXAqCj37QdsBx4XkaHAJ8CvVHUfcA3wuojcgwt8Y8IlICJXAFcAdOnShby8vIgHLC4urnObmnq0aMFRhYXI2j0APP3G0xyTfUxMaSRafcrdHFi5k0uylhv8KbufgUPCLNMo920BDAemq+pHInI/MBP4LXAVcK2q/kNEfoqrlZxyyIFUZwOzAUaMGKHjx4+PeMC8vDzq2uYQlZXw179yfufB3LAR0nukM35EjGkkWL3K3QxYuZNLspYb/Cm7n01VBUCvkOc9ib5ZqQAoUNWPvOcv4AIJwFTgRe/x87gmscQIXlm1YSdt0ttYP4cxJin4GTiWAkeJSF8RSQcuAF6JZkdV3QpsEpH+3qIJQHAkwc3AD73HJwNr45flGHXrBoEAssaurDLGJA/fmqpUtUJErgZeB1KBOaq6UkSmeetniUhXYBmus7tKRK4BclS1CJgOPO0FnQ3AZV7SvwTuF5EWwAG8foyEEHG1jlWrGHTqIBauXZiwrBhjTGPxs48DVV0ILKyxbFbI4624Jqxw+64ARoRZ/j5wbHxz2gA5OfDqqwzqdCaPr3icnSU76dC6Q6JzZYwxvrF/jjfUwIGwdSuDWh8B2NAjxpjmzwJHQ3kd5Dm7UgEb7NAY0/xZ4GgoL3D02rCTrPQsq3EYY5o9CxwN1acPZGTYlVXGmKRhgaOhUlPdHOSrVzOo0yBrqjLGNHsWOOIhZBrZ7SXb2b5ve6JzZIwxvrHAEQ85OZCfz6DAkYBdWWWMad4scMTDwIGgyrF7s0iVVF5b91qic2SMMb6xwBEP3pVVHTdsZeIPJvLU509RWVWZ4EwZY4w/LHDEw1FHuU7yVauYOnQqm/du5u2v3050rowxxhcWOOIhPR1+8ANYvZqz+59N24y2PPHZE4nOlTHG+MICR7x4V1ZltMhg8uDJzF89n6LSokTnyhhj4s4CR7wMHAhr10J5OVOHTmV/xX6eX/l8onNljDFxZ4EjXgYOhIoKWLeOkT1G0r9Df+Z+NjfRuTLGmLizwBEvOTnufvVqRISpQ6fy/jfvs37X+sTmyxhj4swCR7wMGODuV68G4OKhFyMIT372ZAIzZYwx8WeBI14yM6F37+rA0TPQk1P6ncKTnz9JlVYlOHPGGBM/FjjiyZtGNmjq0KnkF+bz3sb3EpgpY4yJLwsc8ZSTA2vWQJWrYfx44I/JSs+y/3QYY5oVCxzxNHAg7N8P33wDQOu01pyfcz7Pr3qefWX7Epw5Y4yJDwsc8eSNWcUXX1Qvmpo7leKyYuavmZ+gTBljTHxZ4Iin4cOhTRt4+eXqRSf0PoG+bfsyd8XcxOXLGGPiyAJHPLVuDeedB88/DwcOAJAiKUwdOpV3vn6HTXs2JTiDxhjTcBY44u2ii2DPHliwoHrRJUMvQVEeXvpwAjNmjDHx4WvgEJGJIvKliKwTkZlh1g8QkSUiUioi19dY11ZEXhCRNSKyWkSOD1k33Ut3pYj8yc8yxOzkk6FrV/jb36oX9W3Xl4uGXMSfPviTXZprjDns+RY4RCQVeAg4HcgBJotITo3NdgEzgHvCJHE/8JqqDgCGAqu9dE8CzgWGqOqgWvZNnNRUuPBCV+PYtat68cNnPEy/dv248MUL2VmyM4EZNMaYhvGzxjESWKeqG1S1DJiH+8GvpqrbVHUpUB66XEQCwDjgMW+7MlUt9FZfBdylqqXBNHwsQ/1cdBGUl8MLL1QvymqZxbM/eZZt+7Zx6cuXoqoJzKAxxtSfn4GjBxDaG1zgLYtGP2A78LiIfCoij4pIprfuaOBEEflIRN4VkePil+U4yc11fwYMaa4CGN5tOHefejf//Oqf3P/R/QnKnDHGNEwLH9OWMMuiPc1uAQwHpqvqRyJyPzAT+K23rh0wGjgOeE5E+mmNU3gRuQK4AqBLly7k5eVFPGBxcXGd28Si95gx9Hv0UT6cN48DXbtWLz9Gj2Fsh7Hc8MYNtNreiv5Z/eN2zPqId7kPF1bu5JKs5Qafyq6qvtyA44HXQ57fDNxcy7a3AdeHPO8K5Ic8PxFY4D1+DRgfsm490ClSXo499lity6JFi+rcJib5+aqgeuedh6zaWbJTe93bS4+8/0jdc2BPfI8bo7iX+zBh5U4uyVpu1YaVHVimYX5T/WyqWgocJSJ9RSQduAB4JZodVXUrsElEgqfjE4Dg6IEvAScDiMjRQDqwI54Zj4sjjoATT4SnnoIa/RntW7Xnmf94hvzCfK7855XW32GMOaz4FjhUtQK4Gngdd0XUc6q6UkSmicg0ABHpKiIFwHXAb0SkwOsYB5gOPC0inwO5wP94y+cA/UTkC1yH+1Rtqr+8F13kBj389NNDVo3tPZbbT7qdeV/Ms/93GGMOK372caCqC4GFNZbNCnm8FehZy74rgBFhlpcBF8U3pz45/3yYPt11kg8ffsjqmSfM5P1v3ufqV69mxdYV3DfxPjLTM8MkZIwxTYf9c9xP7drBmWfCM8+4+chrSJEUXr7gZW454RYe+/Qxhs8ezvItyxOQUWOMiZ4FDr9ddBFs3QrvvBN2dVpqGndOuJN3pr7DvrJ9jH50NH/+4M82a6AxpsmywOG3M86Atm0P+U9HTeP7jOezaZ9xdv+zuf7N65n4t4ls2bulkTJpjDHRs8Dht4wM19fx4ouwL/JkTh1ad+CF819g9lmzef+b9+n/YH+m/XMayzYvsyuvjDFNhgWOxnDRRS5ozK97MicR4ZfH/pJPr/yU8waex5OfPclxjxzH8NnDeejjhyg8UFhnGsYY4ycLHI3hhBNgwAC4+WbYvTuqXfp37M/cSXPZ8ustPHzGw6RICle/ejXd/tyNKS9O4bHlj7Fy20rrCzHGNDpfL8c1npQU18dx/PFwxRXw3HMg4UZkOVR2RjZXHXcVVx13Fcu3LOfR5Y/y7Mpn+fu//w5AoGWA47ofx+ieoxnVYxT9O/and3ZvMlpk+FkiY0wSs8DRWI49Fu68E268EebMgZ//POYkhncbzsNnPsxDZzzE2l1r+bDgw+rbXe/fRaVWVm/btU1X+rTtwxHZR9CnbR+6ZHahfav2B93atWpHaWUpqopEGciMMUaSodN1xIgRumzZsojb5OXlMX78eH8zUlUFP/oRLFkCy5dD//gNcFhSXsKnWz5lw+4N5Bfms3HPxur7jYUbKa8qr3VfQWid1vqgW6u0VqSnph9yS0tJo0VKC1JTUkmVVPdYUqufp0jKQbfUlFQEQUQi3gOHPA7mLdzzQ8pQS+CrbfsNGzZw5JFHRvHK1p3W4WT9hvUc2S+2cjcHyVpugJ5FPZl8+uR67Ssin6jqIX/EthpHY0pJgSefhCFDYPJkF0BatoxL0q3TWjO291jG9h57yLoqraKotIhd+3dV33bv382u/bv4dPWndO3VlZLyEkrKS9hXvq/6cXllOWWVZZSUl1B4oJCyyjLKKsuorKqkoqqCSq086HGVVh10q6yqpFIr3cBo6CH3Cfd1ojOQIBsSnYEESdJy//GYP8Y9TQscja17d9dUde658JvfwN13+37IFEmhbUZb2ma0pV+7fgety9vXCDWtCII13mBACT6uuS70+SFp1BKEItWm3138LuPGjYs5n4e7xe8tZtyJ0Ze7uUjWcgN8+K8P456mBY5EOOcc+M//hHvucU1Xp56a6BwlzEFNUI3YEpSRmkHrtNaNd8AmolVqq6QcDy1Zyw2QKqlxT9Mux02Ue+5xswRecgls357o3BhjTNQscCRKq1Zu8MPdu2HSJNi8OdE5MsaYqFjgSKQhQ+CJJ2DFChg6FBYurHsfY4xJMAscifazn8GyZa7T/Mwz4de/hrKyROfKGGNqZYGjKRg4ED76CP7rv+Dee2HMGFi3LtG5MsaYsCxwNBUZGfDgg24U3Q0bYNgweOwxq30YY5ocCxxNzY9/7Po8cnPhF7+AXr3gpptg7dpE58wYYwALHE1T796QlwcLFrhmqz//GY4+Gk4+2V2JVVqa6BwaY5KY/QGwqUpNdbMHnnGGu1R37lx45BG48EI3o+CYMTB6tLuNHAnZ2YnOsTEmSVjgOBx07w633AIzZ8Lbb8O8efDhh/Dqq6DqhmgfMABGjXIDJx5xxPe3bt3cGFnGGBMnFjgOJykpbniS4BAle/bA0qUuiHz4ofsfyNy5B++Tlub6Sbp0gfbtoV27g+67bt4M27ZB69bf31q1crf0dLd/zfvUVAtGxiQxCxyHs+xsOOUUdwsqLoZvvoH8fNi48fvb9u2uyWvlSti1C4qKABhQ32OLuABS85aS4talpHx/E4l8C6YXy324/MSw/LiSEhckEyVB858ct28fZCbfmE3JWm6A7GnTIM4DmVrgaG7atHFjYOXkRN6uogIKC/nwjTcYPXQolJS42/793z8uL//+Vlb2/X1lpbtVVHz/uLLSzTcSvKm6+8pK97i2G8R+X1Osy4F927aR2blz5NfILwkcaXff9u1kduqUsOMnSrKWG6AyI/6zgfoaOERkInA/kAo8qqp31Vg/AHgcGA7cqqr3hKxrCzwKDAYUuFxVl4Ssvx64G+ikqjv8LEez1KIFdOzIge7dYdCgROem0a3Ky6NzAoeTTxQrd/IpzsuLe5q+BQ4RSQUeAk4FCoClIvKKqq4K2WwXMAOYFCaJ+4HXVPUnIpIOVLcriEgvL91v/Mq/McaY8Pzs4RwJrFPVDapaBswDzg3dQFW3qepS4KB5TUUkAIwDHvO2K1PVwpBN/gLcCE1hGjljjEkufjZV9QA2hTwvAEZFuW8/YDvwuIgMBT4BfqWq+0TkHOBbVf2stnmmAUTkCuAKgC5dupBXR3WtuLi4zm2aIyt3crFyJx8/yu5n4Aj3qx5tDaEFrt9juqp+JCL3AzNF5H+BW4Ef1ZWAqs4GZgOMGDFC65oeNS8vsVOoJoqVO7lYuZOPH2X3s6mqAOgV8rwnEO1sRQVAgap+5D1/ARdIjgT6Ap+JSL6X5nIR6RqXHBtjjKmTn4FjKXCUiPT1OrcvAF6JZkdV3QpsEpH+3qIJwCpV/beqdlbVPqraBxdghnvbG2OMaQS+NVWpaoWIXA28jrscd46qrhSRad76WV5NYRkQAKpE5BogR1WLgOnA017Q2QBc5ldejTHGRM/X/3Go6kJgYY1ls0Ieb8U1N4XbdwUwoo70+zQ8l8YYY2IhmsB/sTYWEdkObKxjs45AMv6R0MqdXKzcyachZT9CVQ/5y31SBI5oiMgyVY1Yw2mOrNzJxcqdfPwouw1xaowxJiYWOIwxxsTEAsf3Zic6Awli5U4uVu7kE/eyWx+HMcaYmFiNwxhjTEwscBhjjIlJ0gcOEZkoIl+KyDoRmZno/PhJROaIyDYR+SJkWXsReVNE1nr37RKZRz+ISC8RWSQiq0VkpYj8ylverMsuIhki8rGIfOaV+/fe8mZdbnDzAYnIpyLyT+95sy8zgIjki8i/RWSFiCzzlsW97EkdOEImmzodyAEmi0gdc64e1uYCE2ssmwm8rapHAW97z5ubCuDXqjoQGA38l/c+N/eylwInq+pQIBeYKCKjaf7lBvgVsDrkeTKUOegkVc0N+e9G3Mue1IGDKCabak5UdTFu1sVQ5wJPeI+fIPxsjIc1Vd2iqsu9x3txPyg9aOZlV6fYe5rm3ZRmXm4R6QmciZt6OqhZl7kOcS97sgeOcJNN9UhQXhKli6puAfcDC3ROcH58JSJ9gGHARyRB2b0mmxXANuBNb6qC5l7u+3AzhFaFLGvuZQ5S4A0R+cSbzA58KLuvgxweBhoy2ZQ5zIhIG+AfwDWqWhRpBsnmQlUrgVwRaQvMF5HBic6Tn0TkLGCbqn4iIuMTnZ8EGKuqm0WkM/CmiKzx4yDJXuNoyGRTzcV3ItINwLvfluD8+EJE0nBB42lVfdFbnBRlB1DVQiAP18fVnMs9FjjHm+htHnCyiPyN5l3maqq62bvfBszHNcfHvezJHjjqPdlUM/IKMNV7PBV4OYF58YW4qsVjwGpVvTdkVbMuu4h08moaiEgr4BRgDc243Kp6s6r29KZcuAB4R1UvohmXOUhEMkUkK/gYN8X2F/hQ9qT/57iInIFrEw1ONnVngrPkGxF5BhiPG2b5O+B3wEvAc0Bv4BvgfFWt2YF+WBORE4D3gH/zfbv3Lbh+jmZbdhEZgusMTcWdJD6nqreLSAeacbmDvKaq61X1rGQos4j0w9UywHVD/F1V7/Sj7EkfOIwxxsQm2ZuqjDHGxMgChzHGmJhY4DDGGBMTCxzGGGNiYoHDGGNMTCxwGNMAIlLpjUQavMVt8DwR6RM6krExTUWyDzliTEPtV9XcRGfCmMZkNQ5jfODNi/BHbz6Mj0XkB97yI0TkbRH53Lvv7S3vIiLzvbkzPhORMV5SqSLyiDefxhveP8ARkRkisspLZ16CimmSlAUOYxqmVY2mqp+FrCtS1ZHAg7jRCfAeP6mqQ4CngQe85Q8A73pzZwwHVnrLjwIeUtVBQCHwH97ymcAwL51pfhXOmHDsn+PGNICIFKtqmzDL83GTKG3wBljcqqodRGQH0E1Vy73lW1S1o4hsB3qqamlIGn1wQ6Ef5T2/CUhT1T+IyGtAMW7ImJdC5t0wxndW4zDGP1rL49q2Cac05HEl3/dLnombvfJY4BMRsf5K02gscBjjn5+F3C/xHn+AG7UVYArwvvf4beAqqJ58KVBboiKSAvRS1UW4CYvaAofUeozxi52lGNMwrbwZ9oJeU9XgJbktReQj3AnaZG/ZDGCOiNwAbAcu85b/CpgtIj/H1SyuArbUcsxU4G8iko2bjOwv3nwbxjQK6+MwxgdeH8cIVd2R6LwYE2/WVGWMMSYmVuMwxhgTE6txGGOMiYkFDmOMMTGxwGGMMSYmFjiMMcbExAKHMcaYmPx/WRBPk0VQf6QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch = range(1,e+1)\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss')\n",
    "plt.plot(epoch, test_loss, 'g', label='Test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test loss(terminate if loss start increasing)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00642552,  0.00755955,  0.00012041, -0.00335043, -0.01309563,\n",
       "          0.00978314,  0.00724319,  0.00418409,  0.0125563 , -0.00701162,\n",
       "          0.00169655, -0.00480346, -0.00173041,  0.00056208,  0.00032075]]),\n",
       " array([-0.03911387]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00642552,  0.00755955,  0.00012041, -0.00335043, -0.01309563,\n",
       "          0.00978314,  0.00724319,  0.00418409,  0.0125563 , -0.00701162,\n",
       "          0.00169655, -0.00480346, -0.00173041,  0.00056208,  0.00032075]]),\n",
       " array([-0.03911387]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight and bias of 50 epochs\n",
    "w_-clf.coef_, b_-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "source": [
    "<p style=\"font-family:'Segoe UI';font-size:16px;color:green\"><b>Answer:</b> I have plotted above</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522133333333334\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy with 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMokBfs3-2PY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522133333333334\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "print(1-np.sum(y_train - pred(w_,b_,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w_,b_,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
