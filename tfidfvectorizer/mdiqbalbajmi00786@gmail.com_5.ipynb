{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def containing_word(dataset,word):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for review in dataset:\n",
    "        if word in review:\n",
    "            count+=1\n",
    "#     print('in containing_word:',count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create tfidf fit function\n",
    "\n",
    "def fit(dataset):\n",
    "    '''\n",
    "    It returns the dictionary of feature names and their idf\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty set to store unique words\n",
    "    # in the corpus to find idf of that word\n",
    "    unique_words = set() \n",
    "    # initialize feature_idf dictionary to return the final result\n",
    "    feature_idf = dict() \n",
    "    \n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset,(list,)):\n",
    "        # first find the unique words in the dataset\n",
    "        for row in dataset:\n",
    "            for word in (row.split()):\n",
    "                unique_words.add(word)\n",
    "        \n",
    "        # how many reviews are there in the dataset\n",
    "        # to find the idf value\n",
    "        no_of_reviews = len(dataset)\n",
    "        \n",
    "        \n",
    "        for word in unique_words:# for each unique word in the reivew\n",
    "                if len(word)<2:# It is found that adjective has no less than 2 words\n",
    "                    continue\n",
    "                    \n",
    "                # otherwise\n",
    "                \n",
    "                # find how many reviews containing the given 'word'\n",
    "                reviews_contain_word = containing_word(dataset,word)\n",
    "                \n",
    "                # calculate the idf value according to the formula in sklearn official documentation\n",
    "                # to overcome the problem of zero division error\n",
    "                idf_value = 1+ math.log((1+no_of_reviews)/(1+reviews_contain_word))\n",
    "                \n",
    "                # storing the value in the dictionary\n",
    "                # key: 'word'\n",
    "                # value: 'idf' of that word\n",
    "                feature_idf[word]=idf_value\n",
    "        \n",
    "        \n",
    "        # sort the dictionary by their keys \n",
    "        # source: geeksforgeeks.org\n",
    "        feature_idf = dict(sorted(feature_idf.items(),key = lambda kv:(kv[0], kv[1])))\n",
    "        return feature_idf # returning unique words and their idf\n",
    "    else:\n",
    "        # if the dataset is not in the list format\n",
    "        print('you need to pass list of sentence')\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus given by AAIC\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    " # fitting the corpus to custom implementation of tfidfvectorizer\n",
    "features = fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# print the features after fitting the corpus to the custom tfidfVectorizer\n",
    "print(list(features.keys()))\n",
    "# features are as same as sklearn's .get_feature_names() returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# print idf of the gained features from the given corpus\n",
    "print(list(features.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary(suggested and restricted) libraries\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making transform function\n",
    "def transform(dataset, vocab):\n",
    "    # initializing lists of rows,columns,values to make sparse matrix\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    \n",
    "    \n",
    "    if  isinstance(dataset, (list,)):# if the dataset given is list\n",
    "        \n",
    "#         # find length of whole datasets(How many reviews are there?)\n",
    "#         no_of_reviews = len(dataset)\n",
    "        \n",
    "        for idx, row in enumerate(tqdm(dataset)):# for each review in the dataset\n",
    "            \n",
    "            word_freq = dict(Counter(row.split()))# find word frequency\n",
    "            \n",
    "            for word in (row.split()): # for each word in the reveiw\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                #find the column index from vocab\n",
    "                idf = vocab.get(word)\n",
    "                #find tf\n",
    "                tf = word_freq[word]/len(row)\n",
    "                #calculate tf_idf\n",
    "                tf_idf = tf*idf\n",
    "                         \n",
    "                #find the column index from vocab.features\n",
    "                col_index = list(vocab.keys()).index(word)\n",
    "                #we are storing the index of the document\n",
    "                rows.append(idx)\n",
    "                #we are storing the dimensions of the word\n",
    "                columns.append(col_index)\n",
    "                # we are storing the tf-idf of the word\n",
    "                values.append(tf_idf)\n",
    "        \n",
    "        # let's normalize our matrix\n",
    "        tfidf_matrix = csr_matrix((values,(rows,columns)),shape=(len(dataset),len(vocab)))\n",
    "        normalized_matrix = normalize(tfidf_matrix)\n",
    "        return normalized_matrix\n",
    "    else:    # else print the error message\n",
    "        print(\"you need to pass the dataset in list format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 2579.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      " 0.38408524 0.         0.38408524]\n",
      "shape: (4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(list(features.keys()))\n",
    "tfidf = transform(corpus,features)\n",
    "print(tfidf.toarray()[0])\n",
    "# print('shape:',tfidf.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features  = fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aailiyah': 6.922918004572872,\n",
       " 'abandoned': 6.922918004572872,\n",
       " 'ability': 6.229770824012927,\n",
       " 'abroad': 6.922918004572872,\n",
       " 'absolutely': 5.3134800921387715,\n",
       " 'abstruse': 6.922918004572872,\n",
       " 'abysmal': 6.517452896464707,\n",
       " 'academy': 6.922918004572872,\n",
       " 'accents': 6.922918004572872,\n",
       " 'accessible': 6.922918004572872,\n",
       " 'acclaimed': 6.922918004572872,\n",
       " 'accolades': 6.922918004572872,\n",
       " 'accurate': 6.517452896464707,\n",
       " 'accurately': 6.922918004572872,\n",
       " 'accused': 6.517452896464707,\n",
       " 'achievement': 6.517452896464707,\n",
       " 'achille': 6.922918004572872,\n",
       " 'ackerman': 6.922918004572872,\n",
       " 'act': 2.6960842593046923,\n",
       " 'acted': 6.229770824012927,\n",
       " 'acting': 3.927185731018881,\n",
       " 'action': 5.218169912334447,\n",
       " 'actions': 6.229770824012927,\n",
       " 'actor': 4.283860674957613,\n",
       " 'actors': 4.671626205966376,\n",
       " 'actress': 5.670155036077504,\n",
       " 'actresses': 6.229770824012927,\n",
       " 'actually': 5.218169912334447,\n",
       " 'adams': 6.922918004572872,\n",
       " 'adaptation': 6.517452896464707,\n",
       " 'add': 5.824305715904762,\n",
       " 'added': 6.922918004572872,\n",
       " 'addition': 6.229770824012927,\n",
       " 'admins': 6.922918004572872,\n",
       " 'admiration': 6.922918004572872,\n",
       " 'admitted': 6.922918004572872,\n",
       " 'adorable': 6.006627272698717,\n",
       " 'adrift': 6.922918004572872,\n",
       " 'adventure': 6.922918004572872,\n",
       " 'advise': 6.517452896464707,\n",
       " 'aerial': 6.229770824012927,\n",
       " 'aesthetically': 6.922918004572872,\n",
       " 'affected': 6.922918004572872,\n",
       " 'affleck': 6.922918004572872,\n",
       " 'afraid': 6.517452896464707,\n",
       " 'africa': 6.517452896464707,\n",
       " 'afternoon': 6.922918004572872,\n",
       " 'age': 4.214867803470662,\n",
       " 'aged': 6.229770824012927,\n",
       " 'ages': 6.229770824012927,\n",
       " 'ago': 5.824305715904762,\n",
       " 'agree': 6.517452896464707,\n",
       " 'agreed': 6.922918004572872,\n",
       " 'aimless': 6.922918004572872,\n",
       " 'air': 5.3134800921387715,\n",
       " 'aired': 6.922918004572872,\n",
       " 'akasha': 6.922918004572872,\n",
       " 'akin': 4.9770078555175585,\n",
       " 'alert': 6.922918004572872,\n",
       " 'alexander': 6.517452896464707,\n",
       " 'alike': 6.922918004572872,\n",
       " 'allison': 6.922918004572872,\n",
       " 'allow': 6.229770824012927,\n",
       " 'allowing': 6.922918004572872,\n",
       " 'almost': 5.3134800921387715,\n",
       " 'along': 6.006627272698717,\n",
       " 'alongside': 6.922918004572872,\n",
       " 'already': 6.006627272698717,\n",
       " 'also': 4.397189360264616,\n",
       " 'although': 6.229770824012927,\n",
       " 'always': 6.006627272698717,\n",
       " 'amateurish': 6.922918004572872,\n",
       " 'amaze': 6.517452896464707,\n",
       " 'amazed': 6.922918004572872,\n",
       " 'amazing': 5.536623643452981,\n",
       " 'amazingly': 6.922918004572872,\n",
       " 'america': 5.670155036077504,\n",
       " 'american': 6.006627272698717,\n",
       " 'americans': 6.517452896464707,\n",
       " 'among': 6.517452896464707,\n",
       " 'amount': 5.824305715904762,\n",
       " 'amusing': 6.922918004572872,\n",
       " 'amust': 6.922918004572872,\n",
       " 'anatomist': 6.922918004572872,\n",
       " 'angel': 5.824305715904762,\n",
       " 'angela': 6.922918004572872,\n",
       " 'angeles': 6.517452896464707,\n",
       " 'angelina': 6.922918004572872,\n",
       " 'angle': 6.229770824012927,\n",
       " 'angles': 6.517452896464707,\n",
       " 'angry': 6.922918004572872,\n",
       " 'anguish': 6.922918004572872,\n",
       " 'angus': 6.922918004572872,\n",
       " 'animals': 6.922918004572872,\n",
       " 'animated': 6.922918004572872,\n",
       " 'animation': 6.517452896464707,\n",
       " 'anita': 6.922918004572872,\n",
       " 'ann': 4.843476462893037,\n",
       " 'anne': 6.006627272698717,\n",
       " 'anniversary': 6.922918004572872,\n",
       " 'annoying': 5.670155036077504,\n",
       " 'another': 5.824305715904762,\n",
       " 'anthony': 6.922918004572872,\n",
       " 'antithesis': 6.922918004572872,\n",
       " 'anyone': 4.843476462893037,\n",
       " 'anything': 5.418840607796598,\n",
       " 'anyway': 6.922918004572872,\n",
       " 'apart': 6.922918004572872,\n",
       " 'appalling': 6.517452896464707,\n",
       " 'appealing': 6.517452896464707,\n",
       " 'appearance': 6.229770824012927,\n",
       " 'appears': 6.922918004572872,\n",
       " 'applauded': 6.922918004572872,\n",
       " 'applause': 6.922918004572872,\n",
       " 'appreciate': 6.006627272698717,\n",
       " 'appropriate': 6.517452896464707,\n",
       " 'apt': 5.824305715904762,\n",
       " 'argued': 6.922918004572872,\n",
       " 'armageddon': 6.922918004572872,\n",
       " 'armand': 6.922918004572872,\n",
       " 'around': 5.824305715904762,\n",
       " 'array': 6.922918004572872,\n",
       " 'art': 3.5217206229107165,\n",
       " 'articulated': 6.922918004572872,\n",
       " 'artiness': 6.922918004572872,\n",
       " 'artist': 6.229770824012927,\n",
       " 'artistic': 6.922918004572872,\n",
       " 'artless': 6.922918004572872,\n",
       " 'arts': 5.418840607796598,\n",
       " 'aside': 6.517452896464707,\n",
       " 'ask': 6.006627272698717,\n",
       " 'asleep': 6.517452896464707,\n",
       " 'aspect': 6.229770824012927,\n",
       " 'aspects': 6.922918004572872,\n",
       " 'ass': 4.480570969203668,\n",
       " 'assante': 6.922918004572872,\n",
       " 'assaulted': 6.922918004572872,\n",
       " 'assistant': 6.922918004572872,\n",
       " 'astonishingly': 6.922918004572872,\n",
       " 'astronaut': 6.922918004572872,\n",
       " 'atmosphere': 6.922918004572872,\n",
       " 'atrocious': 6.922918004572872,\n",
       " 'atrocity': 6.922918004572872,\n",
       " 'attempt': 5.418840607796598,\n",
       " 'attempted': 6.922918004572872,\n",
       " 'attempting': 6.922918004572872,\n",
       " 'attempts': 6.517452896464707,\n",
       " 'attention': 6.517452896464707,\n",
       " 'attractive': 6.922918004572872,\n",
       " 'audience': 5.824305715904762,\n",
       " 'audio': 6.922918004572872,\n",
       " 'aurv': 6.922918004572872,\n",
       " 'austen': 6.922918004572872,\n",
       " 'austere': 6.922918004572872,\n",
       " 'author': 6.922918004572872,\n",
       " 'average': 6.229770824012927,\n",
       " 'aversion': 6.922918004572872,\n",
       " 'avoid': 5.670155036077504,\n",
       " 'avoided': 6.922918004572872,\n",
       " 'award': 6.229770824012927,\n",
       " 'awarded': 6.922918004572872,\n",
       " 'awards': 6.922918004572872,\n",
       " 'away': 5.824305715904762,\n",
       " 'awesome': 6.229770824012927,\n",
       " 'awful': 4.9770078555175585,\n",
       " 'awkwardly': 6.922918004572872,\n",
       " 'aye': 4.725693427236653,\n",
       " 'baaaaaad': 6.922918004572872,\n",
       " 'babbling': 6.922918004572872,\n",
       " 'babie': 6.922918004572872,\n",
       " 'baby': 6.517452896464707,\n",
       " 'babysitting': 6.922918004572872,\n",
       " 'back': 5.418840607796598,\n",
       " 'backdrop': 6.922918004572872,\n",
       " 'backed': 6.922918004572872,\n",
       " 'bad': 3.573013917298267,\n",
       " 'badly': 6.517452896464707,\n",
       " 'bag': 5.418840607796598,\n",
       " 'bailey': 6.922918004572872,\n",
       " 'bakery': 6.922918004572872,\n",
       " 'balance': 6.006627272698717,\n",
       " 'balanced': 6.517452896464707,\n",
       " 'ball': 6.006627272698717,\n",
       " 'ballet': 6.922918004572872,\n",
       " 'balls': 6.922918004572872,\n",
       " 'band': 6.517452896464707,\n",
       " 'barcelona': 6.922918004572872,\n",
       " 'barely': 6.006627272698717,\n",
       " 'barking': 6.922918004572872,\n",
       " 'barney': 6.922918004572872,\n",
       " 'barren': 6.922918004572872,\n",
       " 'based': 6.922918004572872,\n",
       " 'basic': 6.229770824012927,\n",
       " 'basically': 6.517452896464707,\n",
       " 'bat': 6.006627272698717,\n",
       " 'bates': 6.922918004572872,\n",
       " 'baxendale': 6.922918004572872,\n",
       " 'bear': 6.006627272698717,\n",
       " 'beautiful': 5.05111582767128,\n",
       " 'beautifully': 6.517452896464707,\n",
       " 'bec': 5.3134800921387715,\n",
       " 'became': 6.517452896464707,\n",
       " 'bechard': 6.922918004572872,\n",
       " 'become': 5.824305715904762,\n",
       " 'becomes': 6.922918004572872,\n",
       " 'began': 6.517452896464707,\n",
       " 'begin': 5.670155036077504,\n",
       " 'beginning': 6.006627272698717,\n",
       " 'behind': 6.922918004572872,\n",
       " 'behold': 6.922918004572872,\n",
       " 'bela': 6.922918004572872,\n",
       " 'believable': 5.218169912334447,\n",
       " 'believe': 5.3134800921387715,\n",
       " 'believed': 6.922918004572872,\n",
       " 'bell': 6.229770824012927,\n",
       " 'bellucci': 6.922918004572872,\n",
       " 'belly': 6.922918004572872,\n",
       " 'belmondo': 6.922918004572872,\n",
       " 'ben': 6.229770824012927,\n",
       " 'bendingly': 6.922918004572872,\n",
       " 'bennett': 6.922918004572872,\n",
       " 'bergen': 6.922918004572872,\n",
       " 'bertolucci': 6.922918004572872,\n",
       " 'best': 4.525022731774501,\n",
       " 'better': 4.782851841076601,\n",
       " 'betty': 6.517452896464707,\n",
       " 'beware': 6.922918004572872,\n",
       " 'beyond': 5.824305715904762,\n",
       " 'bible': 6.922918004572872,\n",
       " 'big': 5.131158535344817,\n",
       " 'biggest': 6.922918004572872,\n",
       " 'billy': 6.229770824012927,\n",
       " 'biographical': 6.922918004572872,\n",
       " 'bipolarity': 6.922918004572872,\n",
       " 'bit': 5.418840607796598,\n",
       " 'bitchy': 6.922918004572872,\n",
       " 'black': 5.3134800921387715,\n",
       " 'blah': 6.922918004572872,\n",
       " 'blake': 6.922918004572872,\n",
       " 'bland': 6.517452896464707,\n",
       " 'blandly': 6.922918004572872,\n",
       " 'blare': 6.922918004572872,\n",
       " 'blatant': 6.922918004572872,\n",
       " 'blew': 6.922918004572872,\n",
       " 'blood': 6.517452896464707,\n",
       " 'blown': 6.922918004572872,\n",
       " 'blue': 6.922918004572872,\n",
       " 'blush': 6.922918004572872,\n",
       " 'boasts': 6.922918004572872,\n",
       " 'bob': 6.922918004572872,\n",
       " 'body': 5.824305715904762,\n",
       " 'bohemian': 6.922918004572872,\n",
       " 'boiling': 6.922918004572872,\n",
       " 'bold': 6.517452896464707,\n",
       " 'bombardments': 6.922918004572872,\n",
       " 'bond': 6.517452896464707,\n",
       " 'bonding': 6.922918004572872,\n",
       " 'bonus': 6.517452896464707,\n",
       " 'bonuses': 6.922918004572872,\n",
       " 'boobs': 6.922918004572872,\n",
       " 'boogeyman': 6.922918004572872,\n",
       " 'book': 6.006627272698717,\n",
       " 'boost': 6.922918004572872,\n",
       " 'bop': 6.922918004572872,\n",
       " 'bordered': 6.922918004572872,\n",
       " 'borderlines': 6.922918004572872,\n",
       " 'borders': 6.922918004572872,\n",
       " 'bore': 5.536623643452981,\n",
       " 'bored': 6.006627272698717,\n",
       " 'boring': 5.3134800921387715,\n",
       " 'borrowed': 6.922918004572872,\n",
       " 'boss': 6.922918004572872,\n",
       " 'bother': 6.229770824012927,\n",
       " 'bothersome': 6.922918004572872,\n",
       " 'bought': 6.229770824012927,\n",
       " 'box': 6.922918004572872,\n",
       " 'boyfriend': 6.922918004572872,\n",
       " 'boyle': 6.922918004572872,\n",
       " 'brain': 6.229770824012927,\n",
       " 'brainsucking': 6.922918004572872,\n",
       " 'brat': 6.517452896464707,\n",
       " 'breaking': 6.922918004572872,\n",
       " 'breeders': 6.922918004572872,\n",
       " 'brevity': 6.922918004572872,\n",
       " 'brian': 6.517452896464707,\n",
       " 'brief': 6.517452896464707,\n",
       " 'brigand': 6.922918004572872,\n",
       " 'bright': 6.922918004572872,\n",
       " 'brilliance': 6.229770824012927,\n",
       " 'brilliant': 5.536623643452981,\n",
       " 'brilliantly': 6.922918004572872,\n",
       " 'bring': 6.229770824012927,\n",
       " 'brings': 6.922918004572872,\n",
       " 'broad': 6.517452896464707,\n",
       " 'broke': 6.922918004572872,\n",
       " 'brooding': 6.922918004572872,\n",
       " 'brother': 6.922918004572872,\n",
       " 'brutal': 6.922918004572872,\n",
       " 'buddy': 6.922918004572872,\n",
       " 'budget': 5.536623643452981,\n",
       " 'buffalo': 6.922918004572872,\n",
       " 'buffet': 6.922918004572872,\n",
       " 'build': 6.229770824012927,\n",
       " 'builders': 6.922918004572872,\n",
       " 'buildings': 6.922918004572872,\n",
       " 'built': 6.922918004572872,\n",
       " 'bullock': 6.517452896464707,\n",
       " 'bully': 6.922918004572872,\n",
       " 'bunch': 6.517452896464707,\n",
       " 'burton': 6.922918004572872,\n",
       " 'business': 6.517452896464707,\n",
       " 'buy': 6.517452896464707,\n",
       " 'cable': 6.517452896464707,\n",
       " 'cailles': 6.922918004572872,\n",
       " 'california': 6.922918004572872,\n",
       " 'call': 4.9770078555175585,\n",
       " 'called': 6.006627272698717,\n",
       " 'calls': 6.922918004572872,\n",
       " 'came': 4.782851841076601,\n",
       " 'cameo': 6.922918004572872,\n",
       " 'camera': 5.131158535344817,\n",
       " 'camerawork': 6.922918004572872,\n",
       " 'camp': 6.229770824012927,\n",
       " 'campy': 6.922918004572872,\n",
       " 'canada': 6.517452896464707,\n",
       " 'cancan': 6.922918004572872,\n",
       " 'candace': 6.922918004572872,\n",
       " 'candle': 6.922918004572872,\n",
       " 'cannot': 6.517452896464707,\n",
       " 'cant': 6.229770824012927,\n",
       " 'captain': 6.922918004572872,\n",
       " 'captured': 6.922918004572872,\n",
       " 'captures': 6.922918004572872,\n",
       " 'car': 4.214867803470662,\n",
       " 'card': 6.006627272698717,\n",
       " 'cardboard': 6.517452896464707,\n",
       " 'cardellini': 6.922918004572872,\n",
       " 'care': 5.218169912334447,\n",
       " 'carol': 6.922918004572872,\n",
       " 'carrell': 6.922918004572872,\n",
       " 'carries': 6.922918004572872,\n",
       " 'carry': 6.922918004572872,\n",
       " 'cars': 6.922918004572872,\n",
       " 'cartoon': 5.824305715904762,\n",
       " 'cartoons': 6.517452896464707,\n",
       " 'case': 6.229770824012927,\n",
       " 'cases': 6.922918004572872,\n",
       " 'cast': 4.397189360264616,\n",
       " 'casted': 6.922918004572872,\n",
       " 'casting': 5.670155036077504,\n",
       " 'cat': 5.3134800921387715,\n",
       " 'catchy': 6.922918004572872,\n",
       " 'caught': 6.922918004572872,\n",
       " 'cause': 6.517452896464707,\n",
       " 'ceases': 6.517452896464707,\n",
       " 'celebration': 6.922918004572872,\n",
       " 'celebrity': 6.922918004572872,\n",
       " 'celluloid': 6.922918004572872,\n",
       " 'centers': 6.922918004572872,\n",
       " 'central': 6.517452896464707,\n",
       " 'century': 6.517452896464707,\n",
       " 'certain': 5.418840607796598,\n",
       " 'certainly': 5.536623643452981,\n",
       " 'cg': 6.517452896464707,\n",
       " 'cgi': 6.922918004572872,\n",
       " 'chalkboard': 6.922918004572872,\n",
       " 'challenges': 6.922918004572872,\n",
       " 'chance': 6.517452896464707,\n",
       " 'change': 6.006627272698717,\n",
       " 'changes': 6.922918004572872,\n",
       " 'changing': 6.922918004572872,\n",
       " 'channel': 6.922918004572872,\n",
       " 'character': 3.7874237886437223,\n",
       " 'characterisation': 6.922918004572872,\n",
       " 'characters': 4.150329282333091,\n",
       " 'charisma': 6.229770824012927,\n",
       " 'charismatic': 6.922918004572872,\n",
       " 'charles': 6.517452896464707,\n",
       " 'charlie': 6.922918004572872,\n",
       " 'charm': 5.824305715904762,\n",
       " 'charming': 6.229770824012927,\n",
       " 'chase': 6.922918004572872,\n",
       " 'chasing': 6.922918004572872,\n",
       " 'cheap': 5.418840607796598,\n",
       " 'cheaply': 6.922918004572872,\n",
       " 'check': 6.006627272698717,\n",
       " 'checking': 6.229770824012927,\n",
       " 'cheek': 6.517452896464707,\n",
       " 'cheekbones': 6.922918004572872,\n",
       " 'cheerfull': 6.922918004572872,\n",
       " 'cheerless': 6.922918004572872,\n",
       " 'cheesiness': 6.922918004572872,\n",
       " 'cheesy': 6.517452896464707,\n",
       " 'chemistry': 6.006627272698717,\n",
       " 'chick': 6.517452896464707,\n",
       " 'child': 5.131158535344817,\n",
       " 'childhood': 6.517452896464707,\n",
       " 'children': 6.006627272698717,\n",
       " 'childrens': 6.922918004572872,\n",
       " 'chills': 6.922918004572872,\n",
       " 'chilly': 6.922918004572872,\n",
       " 'chimp': 6.922918004572872,\n",
       " 'chodorov': 6.922918004572872,\n",
       " 'choice': 6.922918004572872,\n",
       " 'choices': 6.922918004572872,\n",
       " 'choked': 6.922918004572872,\n",
       " 'chosen': 6.922918004572872,\n",
       " 'chow': 6.922918004572872,\n",
       " 'christmas': 6.517452896464707,\n",
       " 'christopher': 6.922918004572872,\n",
       " 'church': 6.517452896464707,\n",
       " 'cinema': 4.725693427236653,\n",
       " 'cinematic': 6.922918004572872,\n",
       " 'cinematographers': 6.922918004572872,\n",
       " 'cinematography': 5.3134800921387715,\n",
       " 'circumstances': 6.922918004572872,\n",
       " 'class': 5.418840607796598,\n",
       " 'classic': 5.536623643452981,\n",
       " 'classical': 6.922918004572872,\n",
       " 'clear': 6.229770824012927,\n",
       " 'clearly': 6.517452896464707,\n",
       " 'clever': 5.824305715904762,\n",
       " 'clich': 6.006627272698717,\n",
       " 'cliche': 6.922918004572872,\n",
       " 'clients': 6.922918004572872,\n",
       " 'cliff': 6.922918004572872,\n",
       " 'climax': 6.922918004572872,\n",
       " 'close': 6.006627272698717,\n",
       " 'closed': 6.922918004572872,\n",
       " 'clothes': 6.922918004572872,\n",
       " 'club': 6.922918004572872,\n",
       " 'co': 2.406579032291396,\n",
       " 'coach': 6.922918004572872,\n",
       " 'coal': 6.922918004572872,\n",
       " 'coastal': 6.922918004572872,\n",
       " 'coaster': 6.922918004572872,\n",
       " 'coherent': 6.922918004572872,\n",
       " 'cold': 6.922918004572872,\n",
       " 'cole': 6.517452896464707,\n",
       " 'collect': 6.517452896464707,\n",
       " 'collective': 6.922918004572872,\n",
       " 'colored': 6.922918004572872,\n",
       " 'colorful': 6.922918004572872,\n",
       " 'colours': 6.922918004572872,\n",
       " 'columbo': 6.922918004572872,\n",
       " 'come': 4.480570969203668,\n",
       " 'comedic': 6.922918004572872,\n",
       " 'comedy': 5.536623643452981,\n",
       " 'comes': 5.824305715904762,\n",
       " 'comfortable': 6.922918004572872,\n",
       " 'comforting': 6.922918004572872,\n",
       " 'comical': 6.922918004572872,\n",
       " 'coming': 6.229770824012927,\n",
       " 'commands': 6.922918004572872,\n",
       " 'comment': 6.006627272698717,\n",
       " 'commentary': 6.922918004572872,\n",
       " 'commented': 6.922918004572872,\n",
       " 'comments': 6.922918004572872,\n",
       " 'commercial': 6.922918004572872,\n",
       " 'community': 6.922918004572872,\n",
       " 'company': 6.922918004572872,\n",
       " 'compelling': 6.517452896464707,\n",
       " 'competent': 6.922918004572872,\n",
       " 'complete': 5.418840607796598,\n",
       " 'completed': 6.922918004572872,\n",
       " 'completely': 6.006627272698717,\n",
       " 'complex': 6.229770824012927,\n",
       " 'complexity': 6.922918004572872,\n",
       " 'composed': 6.922918004572872,\n",
       " 'composition': 6.517452896464707,\n",
       " 'comprehensible': 6.517452896464707,\n",
       " 'compromise': 6.922918004572872,\n",
       " 'computer': 6.517452896464707,\n",
       " 'concentrate': 6.922918004572872,\n",
       " 'conception': 6.922918004572872,\n",
       " 'conceptually': 6.922918004572872,\n",
       " 'concerning': 6.922918004572872,\n",
       " 'concerns': 6.922918004572872,\n",
       " 'concert': 6.922918004572872,\n",
       " 'conclusion': 6.229770824012927,\n",
       " 'condescends': 6.922918004572872,\n",
       " 'confidence': 6.922918004572872,\n",
       " 'configuration': 6.922918004572872,\n",
       " 'confirm': 6.922918004572872,\n",
       " 'conflict': 6.229770824012927,\n",
       " 'confuses': 6.922918004572872,\n",
       " 'confusing': 6.922918004572872,\n",
       " 'connections': 6.922918004572872,\n",
       " 'connery': 6.517452896464707,\n",
       " 'connor': 6.922918004572872,\n",
       " 'conrad': 6.922918004572872,\n",
       " 'consequences': 6.922918004572872,\n",
       " 'consider': 5.418840607796598,\n",
       " 'considerable': 6.922918004572872,\n",
       " 'considered': 6.922918004572872,\n",
       " 'considering': 6.517452896464707,\n",
       " 'considers': 6.922918004572872,\n",
       " 'consistent': 6.922918004572872,\n",
       " 'consolations': 6.922918004572872,\n",
       " 'constant': 6.517452896464707,\n",
       " 'constantine': 6.922918004572872,\n",
       " 'constructed': 6.922918004572872,\n",
       " 'contained': 6.517452896464707,\n",
       " 'containing': 6.922918004572872,\n",
       " 'contains': 6.517452896464707,\n",
       " 'content': 6.922918004572872,\n",
       " 'continually': 6.922918004572872,\n",
       " 'continuation': 6.922918004572872,\n",
       " 'continue': 6.922918004572872,\n",
       " 'continuity': 6.517452896464707,\n",
       " 'continuously': 6.922918004572872,\n",
       " 'contract': 6.922918004572872,\n",
       " 'contrast': 6.517452896464707,\n",
       " 'contributing': 6.922918004572872,\n",
       " 'contributory': 6.922918004572872,\n",
       " 'contrived': 6.922918004572872,\n",
       " 'control': 6.517452896464707,\n",
       " 'controversy': 6.922918004572872,\n",
       " 'convention': 6.922918004572872,\n",
       " 'convey': 6.922918004572872,\n",
       " 'convince': 6.922918004572872,\n",
       " 'convincing': 5.536623643452981,\n",
       " 'convoluted': 6.922918004572872,\n",
       " 'cool': 5.536623643452981,\n",
       " 'coppola': 6.517452896464707,\n",
       " 'cords': 6.922918004572872,\n",
       " 'core': 5.824305715904762,\n",
       " 'corn': 6.229770824012927,\n",
       " 'corny': 6.922918004572872,\n",
       " 'correct': 6.517452896464707,\n",
       " 'cost': 5.670155036077504,\n",
       " 'costs': 6.229770824012927,\n",
       " 'costumes': 6.922918004572872,\n",
       " 'cotton': 6.922918004572872,\n",
       " 'could': 4.397189360264616,\n",
       " 'couple': 6.922918004572872,\n",
       " 'course': 6.229770824012927,\n",
       " 'court': 6.229770824012927,\n",
       " 'courtroom': 6.517452896464707,\n",
       " 'cover': 5.418840607796598,\n",
       " 'cowardice': 6.922918004572872,\n",
       " 'cox': 6.922918004572872,\n",
       " 'crackles': 6.922918004572872,\n",
       " 'crafted': 6.517452896464707,\n",
       " 'crap': 5.824305715904762,\n",
       " 'crash': 6.517452896464707,\n",
       " 'crashed': 6.922918004572872,\n",
       " 'crayon': 6.517452896464707,\n",
       " 'crayons': 6.922918004572872,\n",
       " 'crazy': 6.517452896464707,\n",
       " 'create': 5.418840607796598,\n",
       " 'created': 6.006627272698717,\n",
       " 'creates': 6.517452896464707,\n",
       " 'creative': 6.922918004572872,\n",
       " 'creativity': 6.922918004572872,\n",
       " 'creature': 6.922918004572872,\n",
       " 'credible': 5.824305715904762,\n",
       " 'credit': 6.229770824012927,\n",
       " 'credits': 6.922918004572872,\n",
       " 'crew': 6.922918004572872,\n",
       " 'crime': 6.922918004572872,\n",
       " 'crisp': 6.922918004572872,\n",
       " 'critic': 6.517452896464707,\n",
       " 'critical': 6.922918004572872,\n",
       " 'crocdodile': 6.922918004572872,\n",
       " 'crocs': 6.922918004572872,\n",
       " 'cross': 6.922918004572872,\n",
       " 'crowd': 6.517452896464707,\n",
       " 'crowe': 6.922918004572872,\n",
       " 'cruel': 6.922918004572872,\n",
       " 'cruise': 6.922918004572872,\n",
       " 'cry': 6.922918004572872,\n",
       " 'cult': 5.670155036077504,\n",
       " 'culture': 6.517452896464707,\n",
       " 'curtain': 6.922918004572872,\n",
       " 'custer': 6.922918004572872,\n",
       " 'cute': 6.006627272698717,\n",
       " 'cutest': 6.922918004572872,\n",
       " 'cutie': 6.922918004572872,\n",
       " 'cutouts': 6.922918004572872,\n",
       " 'cuts': 6.922918004572872,\n",
       " 'cutting': 6.922918004572872,\n",
       " 'dads': 6.922918004572872,\n",
       " 'damian': 6.922918004572872,\n",
       " 'damn': 6.922918004572872,\n",
       " 'dance': 6.006627272698717,\n",
       " 'dancing': 6.517452896464707,\n",
       " 'dangerous': 6.517452896464707,\n",
       " 'dark': 6.517452896464707,\n",
       " 'darren': 6.922918004572872,\n",
       " 'daughter': 6.229770824012927,\n",
       " 'daughters': 6.922918004572872,\n",
       " 'day': 5.131158535344817,\n",
       " 'days': 6.517452896464707,\n",
       " 'de': 2.406579032291396,\n",
       " 'dead': 6.006627272698717,\n",
       " 'deadly': 6.922918004572872,\n",
       " 'deadpan': 6.922918004572872,\n",
       " 'deal': 6.006627272698717,\n",
       " 'dealt': 6.922918004572872,\n",
       " 'death': 6.006627272698717,\n",
       " 'debated': 6.922918004572872,\n",
       " 'debbie': 6.922918004572872,\n",
       " 'debits': 6.922918004572872,\n",
       " 'debut': 6.922918004572872,\n",
       " 'decay': 6.517452896464707,\n",
       " 'decent': 6.229770824012927,\n",
       " 'decidely': 6.922918004572872,\n",
       " 'decipher': 6.922918004572872,\n",
       " 'decisions': 6.922918004572872,\n",
       " 'dedication': 6.517452896464707,\n",
       " 'dee': 5.218169912334447,\n",
       " 'deep': 6.229770824012927,\n",
       " 'deeply': 6.517452896464707,\n",
       " 'defensemen': 6.922918004572872,\n",
       " 'defined': 6.922918004572872,\n",
       " 'definitely': 5.3134800921387715,\n",
       " 'delete': 6.922918004572872,\n",
       " 'delight': 6.006627272698717,\n",
       " 'delightful': 6.922918004572872,\n",
       " 'delights': 6.922918004572872,\n",
       " 'deliver': 5.536623643452981,\n",
       " 'delivered': 6.922918004572872,\n",
       " 'delivering': 6.517452896464707,\n",
       " 'delivers': 6.229770824012927,\n",
       " 'dependant': 6.922918004572872,\n",
       " 'depending': 6.922918004572872,\n",
       " 'depends': 6.922918004572872,\n",
       " 'depicted': 6.922918004572872,\n",
       " 'depicts': 6.517452896464707,\n",
       " 'depressing': 6.517452896464707,\n",
       " 'depth': 6.006627272698717,\n",
       " 'derivative': 6.922918004572872,\n",
       " 'describe': 6.229770824012927,\n",
       " 'describes': 6.922918004572872,\n",
       " 'desert': 6.922918004572872,\n",
       " 'deserved': 6.517452896464707,\n",
       " 'deserves': 6.517452896464707,\n",
       " 'deserving': 6.517452896464707,\n",
       " 'design': 6.229770824012927,\n",
       " 'designed': 6.922918004572872,\n",
       " 'designer': 6.922918004572872,\n",
       " 'desperately': 6.922918004572872,\n",
       " 'desperation': 6.922918004572872,\n",
       " 'despised': 6.922918004572872,\n",
       " 'despite': 6.517452896464707,\n",
       " 'destroy': 6.922918004572872,\n",
       " 'detailing': 6.922918004572872,\n",
       " 'details': 6.922918004572872,\n",
       " 'develop': 6.517452896464707,\n",
       " 'development': 6.517452896464707,\n",
       " 'developments': 6.922918004572872,\n",
       " 'di': 2.681591252002126,\n",
       " 'diabetic': 6.922918004572872,\n",
       " 'dialog': 4.908014984030608,\n",
       " 'dialogs': 6.922918004572872,\n",
       " 'dialogue': 5.131158535344817,\n",
       " 'diaper': 6.922918004572872,\n",
       " 'dickens': 6.922918004572872,\n",
       " 'difference': 6.922918004572872,\n",
       " 'different': 5.824305715904762,\n",
       " 'dignity': 6.922918004572872,\n",
       " 'dimensional': 6.922918004572872,\n",
       " 'direct': 4.438011354784871,\n",
       " 'directed': 6.229770824012927,\n",
       " 'directing': 5.418840607796598,\n",
       " 'direction': 6.006627272698717,\n",
       " 'director': 5.131158535344817,\n",
       " 'directorial': 6.517452896464707,\n",
       " 'directors': 6.922918004572872,\n",
       " 'disappointed': 5.670155036077504,\n",
       " 'disappointing': 6.517452896464707,\n",
       " 'disappointment': 6.517452896464707,\n",
       " 'disaster': 6.517452896464707,\n",
       " 'disbelief': 6.922918004572872,\n",
       " 'discomfort': 6.922918004572872,\n",
       " 'discovering': 6.517452896464707,\n",
       " 'discovery': 6.922918004572872,\n",
       " 'disgrace': 6.922918004572872,\n",
       " 'disgusting': 6.922918004572872,\n",
       " 'dislike': 6.006627272698717,\n",
       " 'disliked': 6.517452896464707,\n",
       " 'disney': 6.922918004572872,\n",
       " 'disparate': 6.922918004572872,\n",
       " 'distant': 6.922918004572872,\n",
       " 'distinction': 6.922918004572872,\n",
       " 'distorted': 6.922918004572872,\n",
       " 'distract': 6.922918004572872,\n",
       " 'distressed': 6.922918004572872,\n",
       " 'disturbing': 6.517452896464707,\n",
       " 'diving': 6.922918004572872,\n",
       " 'doctor': 6.517452896464707,\n",
       " 'documentaries': 6.517452896464707,\n",
       " 'documentary': 6.229770824012927,\n",
       " 'dodge': 6.922918004572872,\n",
       " 'dogs': 6.517452896464707,\n",
       " 'dollars': 6.922918004572872,\n",
       " 'dominated': 6.922918004572872,\n",
       " 'done': 5.536623643452981,\n",
       " 'donlevy': 6.922918004572872,\n",
       " 'dont': 6.922918004572872,\n",
       " 'doomed': 6.922918004572872,\n",
       " 'dose': 6.922918004572872,\n",
       " 'doubt': 6.229770824012927,\n",
       " 'downs': 6.922918004572872,\n",
       " 'dozen': 6.922918004572872,\n",
       " 'dr': 4.0325462466767075,\n",
       " 'dracula': 6.922918004572872,\n",
       " 'draft': 6.922918004572872,\n",
       " 'drag': 6.006627272698717,\n",
       " 'drago': 6.229770824012927,\n",
       " 'drama': 5.131158535344817,\n",
       " 'dramatic': 6.517452896464707,\n",
       " 'drawings': 6.922918004572872,\n",
       " 'drawn': 6.922918004572872,\n",
       " 'dream': 6.229770824012927,\n",
       " 'dreams': 6.517452896464707,\n",
       " 'dreary': 6.922918004572872,\n",
       " 'dribble': 6.922918004572872,\n",
       " 'drift': 6.229770824012927,\n",
       " 'drifting': 6.922918004572872,\n",
       " 'drive': 6.229770824012927,\n",
       " 'drooling': 6.922918004572872,\n",
       " 'dropped': 6.922918004572872,\n",
       " 'dry': 6.922918004572872,\n",
       " 'due': 5.670155036077504,\n",
       " 'duet': 6.517452896464707,\n",
       " 'dull': 6.517452896464707,\n",
       " 'dumb': 6.229770824012927,\n",
       " 'dumbest': 6.922918004572872,\n",
       " 'duper': 6.922918004572872,\n",
       " 'duris': 6.922918004572872,\n",
       " 'dustin': 6.922918004572872,\n",
       " 'dvd': 6.229770824012927,\n",
       " 'dwight': 6.922918004572872,\n",
       " 'dysfunction': 6.922918004572872,\n",
       " 'earlier': 6.229770824012927,\n",
       " 'early': 5.670155036077504,\n",
       " 'earth': 6.517452896464707,\n",
       " 'easily': 6.229770824012927,\n",
       " 'easy': 6.006627272698717,\n",
       " 'eating': 6.517452896464707,\n",
       " 'ebay': 6.922918004572872,\n",
       " 'ebola': 6.922918004572872,\n",
       " 'eccleston': 6.922918004572872,\n",
       " 'ed': 2.0515447778101237,\n",
       " 'edge': 6.229770824012927,\n",
       " 'editing': 5.824305715904762,\n",
       " 'edition': 6.922918004572872,\n",
       " 'educational': 6.922918004572872,\n",
       " 'edward': 6.922918004572872,\n",
       " 'effect': 5.131158535344817,\n",
       " 'effective': 6.229770824012927,\n",
       " 'effects': 5.418840607796598,\n",
       " 'effort': 6.517452896464707,\n",
       " 'efforts': 6.922918004572872,\n",
       " 'egotism': 6.922918004572872,\n",
       " 'eighth': 6.922918004572872,\n",
       " 'eiko': 6.922918004572872,\n",
       " 'either': 6.229770824012927,\n",
       " 'elaborately': 6.922918004572872,\n",
       " 'elderly': 6.922918004572872,\n",
       " 'elegant': 6.922918004572872,\n",
       " 'element': 6.517452896464707,\n",
       " 'elias': 6.922918004572872,\n",
       " 'eloquently': 6.922918004572872,\n",
       " 'else': 5.824305715904762,\n",
       " 'elsewhere': 6.922918004572872,\n",
       " 'embarrassed': 6.922918004572872,\n",
       " 'embarrassing': 6.229770824012927,\n",
       " 'embassy': 6.922918004572872,\n",
       " 'emerge': 6.922918004572872,\n",
       " 'emilio': 6.517452896464707,\n",
       " 'emily': 6.517452896464707,\n",
       " 'emoting': 6.922918004572872,\n",
       " 'emotion': 6.006627272698717,\n",
       " 'emotionally': 6.922918004572872,\n",
       " 'emotions': 6.517452896464707,\n",
       " 'emperor': 6.922918004572872,\n",
       " 'empowerment': 6.922918004572872,\n",
       " 'emptiness': 6.922918004572872,\n",
       " 'empty': 6.517452896464707,\n",
       " 'en': 1.88921743754562,\n",
       " 'enchanting': 6.922918004572872,\n",
       " 'end': 3.505191320959506,\n",
       " 'endearing': 6.922918004572872,\n",
       " 'ended': 5.418840607796598,\n",
       " 'ending': 5.05111582767128,\n",
       " 'endlessly': 6.922918004572872,\n",
       " 'ends': 5.670155036077504,\n",
       " 'energetic': 6.922918004572872,\n",
       " 'energy': 6.006627272698717,\n",
       " 'engaging': 6.517452896464707,\n",
       " 'english': 6.922918004572872,\n",
       " 'enhanced': 6.922918004572872,\n",
       " 'enjoy': 4.843476462893037,\n",
       " 'enjoyable': 6.229770824012927,\n",
       " 'enjoyed': 5.418840607796598,\n",
       " 'enjoyment': 6.922918004572872,\n",
       " 'enough': 5.218169912334447,\n",
       " 'enter': 5.418840607796598,\n",
       " 'enterprise': 6.922918004572872,\n",
       " 'entertained': 6.922918004572872,\n",
       " 'entertaining': 6.006627272698717,\n",
       " 'entire': 5.418840607796598,\n",
       " 'entirely': 6.517452896464707,\n",
       " 'entrance': 6.922918004572872,\n",
       " 'episode': 6.006627272698717,\n",
       " 'episodes': 6.922918004572872,\n",
       " 'equivalent': 6.922918004572872,\n",
       " 'era': 3.854865069439255,\n",
       " 'errol': 6.517452896464707,\n",
       " 'errors': 6.922918004572872,\n",
       " 'escalating': 6.922918004572872,\n",
       " 'escapism': 6.922918004572872,\n",
       " 'especially': 5.418840607796598,\n",
       " 'essence': 6.922918004572872,\n",
       " 'establish': 6.517452896464707,\n",
       " 'established': 6.922918004572872,\n",
       " 'estate': 6.922918004572872,\n",
       " 'estevez': 6.922918004572872,\n",
       " 'etc': 6.229770824012927,\n",
       " 'european': 6.517452896464707,\n",
       " 'evaluate': 6.922918004572872,\n",
       " 'even': 3.9784790254064317,\n",
       " 'events': 6.922918004572872,\n",
       " 'ever': 3.0727704028628136,\n",
       " 'every': 4.089704660516656,\n",
       " 'everybody': 6.922918004572872,\n",
       " 'everyone': 5.418840607796598,\n",
       " 'everything': 5.05111582767128,\n",
       " 'everywhere': 6.922918004572872,\n",
       " 'evidently': 6.922918004572872,\n",
       " 'evil': 6.517452896464707,\n",
       " 'evinced': 6.922918004572872,\n",
       " 'evokes': 6.922918004572872,\n",
       " 'exactly': 6.229770824012927,\n",
       " 'exaggerating': 6.922918004572872,\n",
       " 'example': 6.229770824012927,\n",
       " 'excellent': 4.843476462893037,\n",
       " 'excellently': 6.517452896464707,\n",
       " 'except': 6.006627272698717,\n",
       " 'exceptional': 6.229770824012927,\n",
       " 'exceptionally': 6.922918004572872,\n",
       " 'excerpts': 6.922918004572872,\n",
       " 'excessively': 6.922918004572872,\n",
       " 'exchange': 6.922918004572872,\n",
       " 'exciting': 6.922918004572872,\n",
       " 'excruciatingly': 6.922918004572872,\n",
       " 'excuse': 6.517452896464707,\n",
       " 'excuses': 6.922918004572872,\n",
       " 'executed': 6.922918004572872,\n",
       " 'exemplars': 6.922918004572872,\n",
       " 'existent': 6.517452896464707,\n",
       " 'existential': 6.922918004572872,\n",
       " 'expansive': 6.922918004572872,\n",
       " 'expect': 5.536623643452981,\n",
       " 'expectations': 6.922918004572872,\n",
       " 'expected': 6.922918004572872,\n",
       " 'expecting': 6.922918004572872,\n",
       " 'experience': 5.418840607796598,\n",
       " 'experiences': 6.922918004572872,\n",
       " 'expert': 6.922918004572872,\n",
       " 'explain': 6.229770824012927,\n",
       " 'explains': 6.922918004572872,\n",
       " 'explanation': 6.517452896464707,\n",
       " 'exploit': 6.922918004572872,\n",
       " 'explorations': 6.922918004572872,\n",
       " 'explosion': 6.922918004572872,\n",
       " 'expression': 6.922918004572872,\n",
       " 'exquisite': 6.517452896464707,\n",
       " 'extant': 6.922918004572872,\n",
       " 'exteriors': 6.922918004572872,\n",
       " 'extraneous': 6.922918004572872,\n",
       " 'extraordinary': 6.922918004572872,\n",
       " 'extremely': 6.517452896464707,\n",
       " 'eye': 5.418840607796598,\n",
       " 'eyes': 6.006627272698717,\n",
       " 'fabulous': 6.922918004572872,\n",
       " 'face': 5.670155036077504,\n",
       " 'faces': 6.922918004572872,\n",
       " 'facial': 6.922918004572872,\n",
       " 'facing': 6.922918004572872,\n",
       " 'fact': 5.218169912334447,\n",
       " 'factory': 6.517452896464707,\n",
       " 'failed': 6.922918004572872,\n",
       " 'fails': 5.824305715904762,\n",
       " 'fair': 6.517452896464707,\n",
       " 'fairly': 6.922918004572872,\n",
       " 'faithful': 6.517452896464707,\n",
       " 'fall': 6.006627272698717,\n",
       " 'falling': 6.922918004572872,\n",
       " 'falls': 6.922918004572872,\n",
       " 'falsely': 6.922918004572872,\n",
       " 'falwell': 6.922918004572872,\n",
       " 'fame': 6.517452896464707,\n",
       " 'famed': 6.922918004572872,\n",
       " 'family': 5.536623643452981,\n",
       " 'famous': 6.922918004572872,\n",
       " 'fan': 5.05111582767128,\n",
       " 'fanciful': 6.922918004572872,\n",
       " 'fans': 5.824305715904762,\n",
       " 'fantastic': 6.229770824012927,\n",
       " 'fantasy': 6.922918004572872,\n",
       " 'far': 5.418840607796598,\n",
       " 'farce': 6.922918004572872,\n",
       " 'fare': 6.922918004572872,\n",
       " 'fascinated': 6.922918004572872,\n",
       " 'fascinating': 6.517452896464707,\n",
       " 'fascination': 6.922918004572872,\n",
       " 'fashioned': 6.922918004572872,\n",
       " 'fast': 5.824305715904762,\n",
       " 'faster': 6.922918004572872,\n",
       " 'fat': 6.229770824012927,\n",
       " 'father': 6.922918004572872,\n",
       " 'faultless': 6.922918004572872,\n",
       " 'fausa': 6.922918004572872,\n",
       " 'faux': 6.229770824012927,\n",
       " 'favorite': 6.922918004572872,\n",
       " 'favourite': 6.229770824012927,\n",
       " 'fear': 6.229770824012927,\n",
       " 'feature': 6.006627272698717,\n",
       " 'features': 6.229770824012927,\n",
       " 'feel': 5.05111582767128,\n",
       " 'feeling': 5.3134800921387715,\n",
       " 'feelings': 6.922918004572872,\n",
       " 'feet': 6.922918004572872,\n",
       " 'feisty': 6.922918004572872,\n",
       " 'fellowes': 6.922918004572872,\n",
       " 'felt': 5.824305715904762,\n",
       " 'female': 6.229770824012927,\n",
       " 'females': 6.922918004572872,\n",
       " 'ferry': 6.922918004572872,\n",
       " 'fest': 6.922918004572872,\n",
       " 'fi': 2.369041112972331,\n",
       " 'fields': 6.922918004572872,\n",
       " 'fifteen': 6.922918004572872,\n",
       " 'fifties': 6.922918004572872,\n",
       " 'fill': 6.517452896464707,\n",
       " 'film': 2.6532205548729104,\n",
       " 'filmed': 6.517452896464707,\n",
       " 'filmiing': 6.922918004572872,\n",
       " 'filmmaker': 6.922918004572872,\n",
       " 'filmography': 6.922918004572872,\n",
       " 'films': 4.671626205966376,\n",
       " 'final': 5.536623643452981,\n",
       " 'finale': 6.922918004572872,\n",
       " 'finally': 6.229770824012927,\n",
       " 'financial': 6.922918004572872,\n",
       " 'find': 5.131158535344817,\n",
       " 'finds': 6.922918004572872,\n",
       " 'fine': 5.536623643452981,\n",
       " 'finest': 6.922918004572872,\n",
       " 'fingernails': 6.922918004572872,\n",
       " 'finished': 6.922918004572872,\n",
       " 'fire': 6.922918004572872,\n",
       " 'first': 5.218169912334447,\n",
       " 'fish': 6.517452896464707,\n",
       " 'fishnet': 6.922918004572872,\n",
       " 'fisted': 6.922918004572872,\n",
       " 'fit': 6.922918004572872,\n",
       " 'five': 6.517452896464707,\n",
       " 'flag': 6.922918004572872,\n",
       " 'flakes': 6.922918004572872,\n",
       " 'flaming': 6.922918004572872,\n",
       " 'flashbacks': 6.922918004572872,\n",
       " 'flat': 6.517452896464707,\n",
       " 'flaw': 5.824305715904762,\n",
       " 'flawed': 6.517452896464707,\n",
       " 'flaws': 6.517452896464707,\n",
       " 'fleshed': 6.922918004572872,\n",
       " 'flick': 5.824305715904762,\n",
       " 'flicks': 6.922918004572872,\n",
       " 'florida': 6.922918004572872,\n",
       " 'flowed': 6.922918004572872,\n",
       " 'flying': 6.922918004572872,\n",
       " 'flynn': 6.517452896464707,\n",
       " 'focus': 6.517452896464707,\n",
       " 'fodder': 6.922918004572872,\n",
       " 'follow': 5.670155036077504,\n",
       " 'following': 6.517452896464707,\n",
       " 'follows': 6.922918004572872,\n",
       " 'foolish': 6.922918004572872,\n",
       " 'footage': 6.229770824012927,\n",
       " 'football': 6.922918004572872,\n",
       " 'force': 6.006627272698717,\n",
       " 'forced': 6.922918004572872,\n",
       " 'forces': 6.517452896464707,\n",
       " 'ford': 6.922918004572872,\n",
       " 'foreign': 6.517452896464707,\n",
       " 'foreigner': 6.922918004572872,\n",
       " 'forever': 6.922918004572872,\n",
       " 'forget': 5.824305715904762,\n",
       " 'forgettable': 6.517452896464707,\n",
       " 'forgetting': 6.922918004572872,\n",
       " 'forgot': 6.517452896464707,\n",
       " 'forgotten': 6.922918004572872,\n",
       " 'form': 4.671626205966376,\n",
       " 'format': 6.922918004572872,\n",
       " 'former': 6.922918004572872,\n",
       " ...}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aailiyah',\n",
       " 'abandoned',\n",
       " 'ability',\n",
       " 'abroad',\n",
       " 'absolutely',\n",
       " 'abstruse',\n",
       " 'abysmal',\n",
       " 'academy',\n",
       " 'accents',\n",
       " 'accessible',\n",
       " 'acclaimed',\n",
       " 'accolades',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accused',\n",
       " 'achievement',\n",
       " 'achille',\n",
       " 'ackerman',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'actually',\n",
       " 'adams',\n",
       " 'adaptation',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'admins',\n",
       " 'admiration',\n",
       " 'admitted',\n",
       " 'adorable',\n",
       " 'adrift',\n",
       " 'adventure',\n",
       " 'advise',\n",
       " 'aerial',\n",
       " 'aesthetically',\n",
       " 'affected',\n",
       " 'affleck',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ages',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'aimless',\n",
       " 'air',\n",
       " 'aired',\n",
       " 'akasha',\n",
       " 'akin',\n",
       " 'alert',\n",
       " 'alexander',\n",
       " 'alike',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amateurish',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'among',\n",
       " 'amount',\n",
       " 'amusing',\n",
       " 'amust',\n",
       " 'anatomist',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelina',\n",
       " 'angle',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'angus',\n",
       " 'animals',\n",
       " 'animated',\n",
       " 'animation',\n",
       " 'anita',\n",
       " 'ann',\n",
       " 'anne',\n",
       " 'anniversary',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'anthony',\n",
       " 'antithesis',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'apart',\n",
       " 'appalling',\n",
       " 'appealing',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'applauded',\n",
       " 'applause',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'apt',\n",
       " 'argued',\n",
       " 'armageddon',\n",
       " 'armand',\n",
       " 'around',\n",
       " 'array',\n",
       " 'art',\n",
       " 'articulated',\n",
       " 'artiness',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artless',\n",
       " 'arts',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'ass',\n",
       " 'assante',\n",
       " 'assaulted',\n",
       " 'assistant',\n",
       " 'astonishingly',\n",
       " 'astronaut',\n",
       " 'atmosphere',\n",
       " 'atrocious',\n",
       " 'atrocity',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attempts',\n",
       " 'attention',\n",
       " 'attractive',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'aurv',\n",
       " 'austen',\n",
       " 'austere',\n",
       " 'author',\n",
       " 'average',\n",
       " 'aversion',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awkwardly',\n",
       " 'aye',\n",
       " 'baaaaaad',\n",
       " 'babbling',\n",
       " 'babie',\n",
       " 'baby',\n",
       " 'babysitting',\n",
       " 'back',\n",
       " 'backdrop',\n",
       " 'backed',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'bailey',\n",
       " 'bakery',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'ball',\n",
       " 'ballet',\n",
       " 'balls',\n",
       " 'band',\n",
       " 'barcelona',\n",
       " 'barely',\n",
       " 'barking',\n",
       " 'barney',\n",
       " 'barren',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'bat',\n",
       " 'bates',\n",
       " 'baxendale',\n",
       " 'bear',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'bec',\n",
       " 'became',\n",
       " 'bechard',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'bela',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'bell',\n",
       " 'bellucci',\n",
       " 'belly',\n",
       " 'belmondo',\n",
       " 'ben',\n",
       " 'bendingly',\n",
       " 'bennett',\n",
       " 'bergen',\n",
       " 'bertolucci',\n",
       " 'best',\n",
       " 'better',\n",
       " 'betty',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bible',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'billy',\n",
       " 'biographical',\n",
       " 'bipolarity',\n",
       " 'bit',\n",
       " 'bitchy',\n",
       " 'black',\n",
       " 'blah',\n",
       " 'blake',\n",
       " 'bland',\n",
       " 'blandly',\n",
       " 'blare',\n",
       " 'blatant',\n",
       " 'blew',\n",
       " 'blood',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'blush',\n",
       " 'boasts',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'bohemian',\n",
       " 'boiling',\n",
       " 'bold',\n",
       " 'bombardments',\n",
       " 'bond',\n",
       " 'bonding',\n",
       " 'bonus',\n",
       " 'bonuses',\n",
       " 'boobs',\n",
       " 'boogeyman',\n",
       " 'book',\n",
       " 'boost',\n",
       " 'bop',\n",
       " 'bordered',\n",
       " 'borderlines',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'borrowed',\n",
       " 'boss',\n",
       " 'bother',\n",
       " 'bothersome',\n",
       " 'bought',\n",
       " 'box',\n",
       " 'boyfriend',\n",
       " 'boyle',\n",
       " 'brain',\n",
       " 'brainsucking',\n",
       " 'brat',\n",
       " 'breaking',\n",
       " 'breeders',\n",
       " 'brevity',\n",
       " 'brian',\n",
       " 'brief',\n",
       " 'brigand',\n",
       " 'bright',\n",
       " 'brilliance',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'broad',\n",
       " 'broke',\n",
       " 'brooding',\n",
       " 'brother',\n",
       " 'brutal',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buffalo',\n",
       " 'buffet',\n",
       " 'build',\n",
       " 'builders',\n",
       " 'buildings',\n",
       " 'built',\n",
       " 'bullock',\n",
       " 'bully',\n",
       " 'bunch',\n",
       " 'burton',\n",
       " 'business',\n",
       " 'buy',\n",
       " 'cable',\n",
       " 'cailles',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'cameo',\n",
       " 'camera',\n",
       " 'camerawork',\n",
       " 'camp',\n",
       " 'campy',\n",
       " 'canada',\n",
       " 'cancan',\n",
       " 'candace',\n",
       " 'candle',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'captain',\n",
       " 'captured',\n",
       " 'captures',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardboard',\n",
       " 'cardellini',\n",
       " 'care',\n",
       " 'carol',\n",
       " 'carrell',\n",
       " 'carries',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'cartoon',\n",
       " 'cartoons',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cast',\n",
       " 'casted',\n",
       " 'casting',\n",
       " 'cat',\n",
       " 'catchy',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'ceases',\n",
       " 'celebration',\n",
       " 'celebrity',\n",
       " 'celluloid',\n",
       " 'centers',\n",
       " 'central',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'cg',\n",
       " 'cgi',\n",
       " 'chalkboard',\n",
       " 'challenges',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'character',\n",
       " 'characterisation',\n",
       " 'characters',\n",
       " 'charisma',\n",
       " 'charismatic',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'charm',\n",
       " 'charming',\n",
       " 'chase',\n",
       " 'chasing',\n",
       " 'cheap',\n",
       " 'cheaply',\n",
       " 'check',\n",
       " 'checking',\n",
       " 'cheek',\n",
       " 'cheekbones',\n",
       " 'cheerfull',\n",
       " 'cheerless',\n",
       " 'cheesiness',\n",
       " 'cheesy',\n",
       " 'chemistry',\n",
       " 'chick',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'children',\n",
       " 'childrens',\n",
       " 'chills',\n",
       " 'chilly',\n",
       " 'chimp',\n",
       " 'chodorov',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choked',\n",
       " 'chosen',\n",
       " 'chow',\n",
       " 'christmas',\n",
       " 'christopher',\n",
       " 'church',\n",
       " 'cinema',\n",
       " 'cinematic',\n",
       " 'cinematographers',\n",
       " 'cinematography',\n",
       " 'circumstances',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classical',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clever',\n",
       " 'clich',\n",
       " 'cliche',\n",
       " 'clients',\n",
       " 'cliff',\n",
       " 'climax',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'clothes',\n",
       " 'club',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coal',\n",
       " 'coastal',\n",
       " 'coaster',\n",
       " 'coherent',\n",
       " 'cold',\n",
       " 'cole',\n",
       " 'collect',\n",
       " 'collective',\n",
       " 'colored',\n",
       " 'colorful',\n",
       " 'colours',\n",
       " 'columbo',\n",
       " 'come',\n",
       " 'comedic',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'comforting',\n",
       " 'comical',\n",
       " 'coming',\n",
       " 'commands',\n",
       " 'comment',\n",
       " 'commentary',\n",
       " 'commented',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compelling',\n",
       " 'competent',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complexity',\n",
       " 'composed',\n",
       " 'composition',\n",
       " 'comprehensible',\n",
       " 'compromise',\n",
       " 'computer',\n",
       " 'concentrate',\n",
       " 'conception',\n",
       " 'conceptually',\n",
       " 'concerning',\n",
       " 'concerns',\n",
       " 'concert',\n",
       " 'conclusion',\n",
       " 'condescends',\n",
       " 'confidence',\n",
       " 'configuration',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'confuses',\n",
       " 'confusing',\n",
       " 'connections',\n",
       " 'connery',\n",
       " 'connor',\n",
       " 'conrad',\n",
       " 'consequences',\n",
       " 'consider',\n",
       " 'considerable',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'considers',\n",
       " 'consistent',\n",
       " 'consolations',\n",
       " 'constant',\n",
       " 'constantine',\n",
       " 'constructed',\n",
       " 'contained',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'content',\n",
       " 'continually',\n",
       " 'continuation',\n",
       " 'continue',\n",
       " 'continuity',\n",
       " 'continuously',\n",
       " 'contract',\n",
       " 'contrast',\n",
       " 'contributing',\n",
       " 'contributory',\n",
       " 'contrived',\n",
       " 'control',\n",
       " 'controversy',\n",
       " 'convention',\n",
       " 'convey',\n",
       " 'convince',\n",
       " 'convincing',\n",
       " 'convoluted',\n",
       " 'cool',\n",
       " 'coppola',\n",
       " 'cords',\n",
       " 'core',\n",
       " 'corn',\n",
       " 'corny',\n",
       " 'correct',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'costumes',\n",
       " 'cotton',\n",
       " 'could',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'courtroom',\n",
       " 'cover',\n",
       " 'cowardice',\n",
       " 'cox',\n",
       " 'crackles',\n",
       " 'crafted',\n",
       " 'crap',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'crayon',\n",
       " 'crayons',\n",
       " 'crazy',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'creative',\n",
       " 'creativity',\n",
       " 'creature',\n",
       " 'credible',\n",
       " 'credit',\n",
       " 'credits',\n",
       " 'crew',\n",
       " 'crime',\n",
       " 'crisp',\n",
       " 'critic',\n",
       " 'critical',\n",
       " 'crocdodile',\n",
       " 'crocs',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'crowe',\n",
       " 'cruel',\n",
       " 'cruise',\n",
       " 'cry',\n",
       " 'cult',\n",
       " 'culture',\n",
       " 'curtain',\n",
       " 'custer',\n",
       " 'cute',\n",
       " 'cutest',\n",
       " 'cutie',\n",
       " 'cutouts',\n",
       " 'cuts',\n",
       " 'cutting',\n",
       " 'dads',\n",
       " 'damian',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'dangerous',\n",
       " 'dark',\n",
       " 'darren',\n",
       " 'daughter',\n",
       " 'daughters',\n",
       " 'day',\n",
       " 'days',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deadly',\n",
       " 'deadpan',\n",
       " 'deal',\n",
       " 'dealt',\n",
       " 'death',\n",
       " 'debated',\n",
       " 'debbie',\n",
       " 'debits',\n",
       " 'debut',\n",
       " 'decay',\n",
       " 'decent',\n",
       " 'decidely',\n",
       " 'decipher',\n",
       " 'decisions',\n",
       " 'dedication',\n",
       " 'dee',\n",
       " 'deep',\n",
       " 'deeply',\n",
       " 'defensemen',\n",
       " 'defined',\n",
       " 'definitely',\n",
       " 'delete',\n",
       " 'delight',\n",
       " 'delightful',\n",
       " 'delights',\n",
       " 'deliver',\n",
       " 'delivered',\n",
       " 'delivering',\n",
       " 'delivers',\n",
       " 'dependant',\n",
       " 'depending',\n",
       " 'depends',\n",
       " 'depicted',\n",
       " 'depicts',\n",
       " 'depressing',\n",
       " 'depth',\n",
       " 'derivative',\n",
       " 'describe',\n",
       " 'describes',\n",
       " 'desert',\n",
       " 'deserved',\n",
       " 'deserves',\n",
       " 'deserving',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'designer',\n",
       " 'desperately',\n",
       " 'desperation',\n",
       " 'despised',\n",
       " 'despite',\n",
       " 'destroy',\n",
       " 'detailing',\n",
       " 'details',\n",
       " 'develop',\n",
       " 'development',\n",
       " 'developments',\n",
       " 'di',\n",
       " 'diabetic',\n",
       " 'dialog',\n",
       " 'dialogs',\n",
       " 'dialogue',\n",
       " 'diaper',\n",
       " 'dickens',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'dignity',\n",
       " 'dimensional',\n",
       " 'direct',\n",
       " 'directed',\n",
       " 'directing',\n",
       " 'direction',\n",
       " 'director',\n",
       " 'directorial',\n",
       " 'directors',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disappointment',\n",
       " 'disaster',\n",
       " 'disbelief',\n",
       " 'discomfort',\n",
       " 'discovering',\n",
       " 'discovery',\n",
       " 'disgrace',\n",
       " 'disgusting',\n",
       " 'dislike',\n",
       " 'disliked',\n",
       " 'disney',\n",
       " 'disparate',\n",
       " 'distant',\n",
       " 'distinction',\n",
       " 'distorted',\n",
       " 'distract',\n",
       " 'distressed',\n",
       " 'disturbing',\n",
       " 'diving',\n",
       " 'doctor',\n",
       " 'documentaries',\n",
       " 'documentary',\n",
       " 'dodge',\n",
       " 'dogs',\n",
       " 'dollars',\n",
       " 'dominated',\n",
       " 'done',\n",
       " 'donlevy',\n",
       " 'dont',\n",
       " 'doomed',\n",
       " 'dose',\n",
       " 'doubt',\n",
       " 'downs',\n",
       " 'dozen',\n",
       " 'dr',\n",
       " 'dracula',\n",
       " 'draft',\n",
       " 'drag',\n",
       " 'drago',\n",
       " 'drama',\n",
       " 'dramatic',\n",
       " 'drawings',\n",
       " 'drawn',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'dreary',\n",
       " 'dribble',\n",
       " 'drift',\n",
       " 'drifting',\n",
       " 'drive',\n",
       " 'drooling',\n",
       " 'dropped',\n",
       " 'dry',\n",
       " 'due',\n",
       " 'duet',\n",
       " 'dull',\n",
       " 'dumb',\n",
       " 'dumbest',\n",
       " 'duper',\n",
       " 'duris',\n",
       " 'dustin',\n",
       " 'dvd',\n",
       " 'dwight',\n",
       " 'dysfunction',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'eating',\n",
       " 'ebay',\n",
       " 'ebola',\n",
       " 'eccleston',\n",
       " 'ed',\n",
       " 'edge',\n",
       " 'editing',\n",
       " 'edition',\n",
       " 'educational',\n",
       " 'edward',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'efforts',\n",
       " 'egotism',\n",
       " 'eighth',\n",
       " 'eiko',\n",
       " 'either',\n",
       " 'elaborately',\n",
       " 'elderly',\n",
       " 'elegant',\n",
       " 'element',\n",
       " 'elias',\n",
       " 'eloquently',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'embarrassed',\n",
       " 'embarrassing',\n",
       " 'embassy',\n",
       " 'emerge',\n",
       " 'emilio',\n",
       " 'emily',\n",
       " 'emoting',\n",
       " 'emotion',\n",
       " 'emotionally',\n",
       " 'emotions',\n",
       " 'emperor',\n",
       " 'empowerment',\n",
       " 'emptiness',\n",
       " 'empty',\n",
       " 'en',\n",
       " 'enchanting',\n",
       " 'end',\n",
       " 'endearing',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'endlessly',\n",
       " 'ends',\n",
       " 'energetic',\n",
       " 'energy',\n",
       " 'engaging',\n",
       " 'english',\n",
       " 'enhanced',\n",
       " 'enjoy',\n",
       " 'enjoyable',\n",
       " 'enjoyed',\n",
       " 'enjoyment',\n",
       " 'enough',\n",
       " 'enter',\n",
       " 'enterprise',\n",
       " 'entertained',\n",
       " 'entertaining',\n",
       " 'entire',\n",
       " 'entirely',\n",
       " 'entrance',\n",
       " 'episode',\n",
       " 'episodes',\n",
       " 'equivalent',\n",
       " 'era',\n",
       " 'errol',\n",
       " 'errors',\n",
       " 'escalating',\n",
       " 'escapism',\n",
       " 'especially',\n",
       " 'essence',\n",
       " 'establish',\n",
       " 'established',\n",
       " 'estate',\n",
       " 'estevez',\n",
       " 'etc',\n",
       " 'european',\n",
       " 'evaluate',\n",
       " 'even',\n",
       " 'events',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'evidently',\n",
       " 'evil',\n",
       " 'evinced',\n",
       " 'evokes',\n",
       " 'exactly',\n",
       " 'exaggerating',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'excellently',\n",
       " 'except',\n",
       " 'exceptional',\n",
       " 'exceptionally',\n",
       " 'excerpts',\n",
       " 'excessively',\n",
       " 'exchange',\n",
       " 'exciting',\n",
       " 'excruciatingly',\n",
       " 'excuse',\n",
       " 'excuses',\n",
       " 'executed',\n",
       " 'exemplars',\n",
       " 'existent',\n",
       " 'existential',\n",
       " 'expansive',\n",
       " 'expect',\n",
       " 'expectations',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'experience',\n",
       " 'experiences',\n",
       " 'expert',\n",
       " 'explain',\n",
       " 'explains',\n",
       " 'explanation',\n",
       " 'exploit',\n",
       " 'explorations',\n",
       " 'explosion',\n",
       " 'expression',\n",
       " 'exquisite',\n",
       " 'extant',\n",
       " 'exteriors',\n",
       " 'extraneous',\n",
       " 'extraordinary',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'faces',\n",
       " 'facial',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'factory',\n",
       " 'failed',\n",
       " 'fails',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'faithful',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'falls',\n",
       " 'falsely',\n",
       " 'falwell',\n",
       " 'fame',\n",
       " 'famed',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'fanciful',\n",
       " 'fans',\n",
       " 'fantastic',\n",
       " 'fantasy',\n",
       " 'far',\n",
       " 'farce',\n",
       " 'fare',\n",
       " 'fascinated',\n",
       " 'fascinating',\n",
       " 'fascination',\n",
       " 'fashioned',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fat',\n",
       " 'father',\n",
       " 'faultless',\n",
       " 'fausa',\n",
       " 'faux',\n",
       " 'favorite',\n",
       " 'favourite',\n",
       " 'fear',\n",
       " 'feature',\n",
       " 'features',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feelings',\n",
       " 'feet',\n",
       " 'feisty',\n",
       " 'fellowes',\n",
       " 'felt',\n",
       " 'female',\n",
       " 'females',\n",
       " 'ferry',\n",
       " 'fest',\n",
       " 'fi',\n",
       " 'fields',\n",
       " 'fifteen',\n",
       " 'fifties',\n",
       " 'fill',\n",
       " 'film',\n",
       " 'filmed',\n",
       " 'filmiing',\n",
       " 'filmmaker',\n",
       " 'filmography',\n",
       " 'films',\n",
       " 'final',\n",
       " 'finale',\n",
       " 'finally',\n",
       " 'financial',\n",
       " 'find',\n",
       " 'finds',\n",
       " 'fine',\n",
       " 'finest',\n",
       " 'fingernails',\n",
       " 'finished',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'fishnet',\n",
       " 'fisted',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'flag',\n",
       " 'flakes',\n",
       " 'flaming',\n",
       " 'flashbacks',\n",
       " 'flat',\n",
       " 'flaw',\n",
       " 'flawed',\n",
       " 'flaws',\n",
       " 'fleshed',\n",
       " 'flick',\n",
       " 'flicks',\n",
       " 'florida',\n",
       " 'flowed',\n",
       " 'flying',\n",
       " 'flynn',\n",
       " 'focus',\n",
       " 'fodder',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'foolish',\n",
       " 'footage',\n",
       " 'football',\n",
       " 'force',\n",
       " 'forced',\n",
       " 'forces',\n",
       " 'ford',\n",
       " 'foreign',\n",
       " 'foreigner',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgettable',\n",
       " 'forgetting',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'format',\n",
       " 'former',\n",
       " ...]"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.922918  , 6.922918  , 6.22977082, ..., 6.922918  , 6.5174529 ,\n",
       "       6.922918  ])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Segoe UI'>\n",
    "    <h4><strong>Implement max features functionality:</strong></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sort_dictionary_by_value(dictionary, t_reverse=False):\n",
    "    '''\n",
    "    This function reverse the dictionary according to the value in ascending or descending order\n",
    "    this function returns the sorted dictionary \n",
    "    >>> temp_dict = {'a':20,'b':3,'c':5,'d':8}\n",
    "    >>> sorted_dict = sort_dictionary_by_value(temp_dict,True)\n",
    "    >>> sorted_dict\n",
    "    O[1]: {'a':20,'d':8,'c':5,'b':3}\n",
    "    '''\n",
    "    # change the dictionary into list of tuples of dictionary's values and keys\n",
    "    # step1-> access dictionary's values\n",
    "    # step2-> change step1 to list\n",
    "    # step3-> access dictionary's keys\n",
    "    # step4-> change step 3 to list\n",
    "    # step5-> zip step2 and step4(values and keys lists)\n",
    "    # step6-> typecast it to list again\n",
    "    temp_list = list(zip(list(dictionary.values()),list(dictionary.keys())))\n",
    "    \n",
    "    # Now, let's sort the list\n",
    "    temp_list.sort(reverse = t_reverse)\n",
    "    \n",
    "    for i, item in enumerate(temp_list):\n",
    "        temp = list(item)\n",
    "        temp.reverse()\n",
    "        temp = tuple(temp)\n",
    "        # override the temp_list\n",
    "        temp_list[i] = temp\n",
    "        \n",
    "    t_list = temp_list[0:50]\n",
    "    \n",
    "    again_dict = dict(t_list)\n",
    "    \n",
    "    return again_dict# returning the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create tfidf fit function\n",
    "\n",
    "# let's do some change in fit function.\n",
    "# impelement maximum feature functionality\n",
    "def fit(dataset):\n",
    "    '''\n",
    "    It returns the dictionary of feature names and their idf\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty set to store unique words\n",
    "    # in the corpus to find idf of that word\n",
    "    unique_words = set() \n",
    "    # initialize feature_idf dictionary to return the final result\n",
    "    feature_idf = dict() \n",
    "    \n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset,(list,)):\n",
    "        # first find the unique words in the dataset\n",
    "        for row in dataset:\n",
    "            for word in (row.split()):\n",
    "                unique_words.add(word)\n",
    "        \n",
    "        print(unique_words)\n",
    "        # how many reviews are there in the dataset\n",
    "        # to find the idf value\n",
    "        no_of_reviews = len(dataset)\n",
    "        \n",
    "        \n",
    "        for word in unique_words:# for each unique word in the reivew\n",
    "                if len(word)<2:# It is found that adjective has no less than 2 words\n",
    "                    continue\n",
    "                    \n",
    "                # otherwise\n",
    "                \n",
    "                # find how many reviews containing the given 'word'\n",
    "                reviews_contain_word = containing_word(dataset,word)\n",
    "                \n",
    "                # calculate the idf value according to the formula in sklearn official documentation\n",
    "                # to overcome the problem of zero division error\n",
    "                idf_value = 1+ math.log((1+no_of_reviews)/(1+reviews_contain_word))\n",
    "                \n",
    "                # storing the value in the dictionary\n",
    "                # key: 'word'\n",
    "                # value: 'idf' of that word\n",
    "                feature_idf[word]=idf_value\n",
    "\n",
    "        # sort dictionary by_value_return top 50 features\n",
    "        sorted_feature_idf = sort_dictionary_by_value(feature_idf, True)\n",
    "        \n",
    "        #Now, sort according to their keys\n",
    "        new_feature_idf = dict(sorted(sorted_feature_idf.items(),key = lambda kv:(kv[0], kv[1])))\n",
    "        \n",
    "        return new_feature_idf # returning unique words and their idf\n",
    "    else:\n",
    "        # if the dataset is not in the list format\n",
    "        print('you need to pass list of sentence')\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'terribly', 'witticisms', 'interacting', 'giallo', 'par', 'getting', 'crocs', 'genius', 'perfected', 'ole', 'granted', 'spacey', 'poler', 'despite', 'villains', 'rated', 'decisions', 'finished', 'towers', 'kept', 'eiko', 'crayon', 'anne', 'cast', 'politics', 'consider', 'look', 'adventure', 'portrayal', 'bela', 'gung', 'god', 'indeed', 'june', 'unconvincing', 'connery', 'commands', 'spend', 'underwater', 'car', 'identified', 'coming', 'weird', 'wide', 'daughters', 'unwatchable', 'welsh', 'nobody', 'regrettable', 'chalkboard', 'monstrous', 'unneeded', 'tough', 'fare', 'unique', 'sappiest', 'holds', 'vey', 'constantine', 'painful', 'dr', 'lies', 'road', 'z', 'salesman', 'convincing', 'four', 'fundamental', 'mystifying', 'clients', 'understanding', 'performances', 'documentary', 'buddy', 'boring', 'timeless', 'foreigner', 'contrast', 'kudos', 'finale', 'fascination', 'captures', 'resounding', 'happiness', 'terrific', 'scares', 'forced', 'whether', 'houses', 'blue', 'situation', 'pandering', 'visually', 'ebay', 'opinion', 'totally', 'insincere', 'references', 'victor', 'producer', 'issue', 'considerable', 'rips', 'asleep', 'vomit', 'actor', 'instead', 'exquisite', 'saw', 'astronaut', 'main', 'white', 'colorful', 'kinda', 'overall', 'hoot', 'must', 'pointless', 'ready', 'smith', 'contrived', 'recently', 'using', 'taped', 'hurt', 'cry', 'deadpan', 'shallow', 'span', 'walked', 'documentaries', 'depending', 'waste', 'cinematographers', 'throwback', 'en', 'rochon', 'others', 'flag', 'race', 'thoroughly', 'stoic', 'boyle', 'spacek', 'borderlines', 'tone', 'gross', 'share', 'timely', 'cant', 'animation', 'nonsense', 'havilland', 'unoriginal', 'foreign', 'stowe', 'settings', 'everywhere', 'flaws', 'function', 'offensive', 'thread', 'video', 'eye', 'mishima', 'uninteresting', 'popular', 'aside', 'laughed', 'lesser', 'fifteen', 'war', 'cancan', 'mouth', 'unconditional', 'days', 'understand', 'innocence', 'fan', 'place', 'chasing', 'seeing', 'pap', 'child', 'subjects', 'insulin', 'felt', 'joke', 'identify', 'competent', 'among', 'operas', 'goth', 'distinction', 'drago', 'tongue', 'grasp', 'imagine', 'willie', 'blown', 'intensity', 'vision', 'point', 'cartoon', 'greenstreet', 'simply', 'ryan', 'teaches', 'foolish', 'unfortunately', 'flying', 'previous', 'hopeless', 'educational', 'community', 'left', 'stupidity', 'pieces', 'kanaly', 'cause', 'completed', 'dialogs', 'heart', 'revealing', 'possible', 'sign', 'continuation', 'ps', 'disturbing', 'universe', 'destroy', 'nasty', 'expectations', 'relations', 'changing', 'nun', 'value', 'members', 'feeling', 'network', 'suspense', 'oh', 'teddy', 'tear', 'public', 'depth', 'amazing', 'subtle', 'pledge', 'enchanting', 'judging', 'giovanni', 'opening', 'neil', 'surprisingly', 'owed', 'coach', 'woo', 'predictable', 'killing', 'wants', 'financial', 'shelves', 'incredibly', 'two', 'schrader', 'idealogical', 'hendrikson', 'slideshow', 'total', 'recurring', 'hopefully', 'necklace', 'gave', 'variation', 'scenery', 'surf', 'therapy', 'pretentious', 'challenges', 'scenes', 'co', 'stunning', 'akin', 'diabetic', 'guys', 'sequence', 'dependant', 'throughout', 'debbie', 'miserably', 'manages', 'creates', 'playing', 'believe', 'steele', 'campy', 'helen', 'levels', 'zillion', 'sophisticated', 'concerning', 'check', 'sweep', 'hatred', 'superbad', 'history', 'yardley', 'ironically', 'basically', 'win', 'huge', 'needlessly', 'pleasant', 'mention', 'yawn', 'represents', 'charismatic', 'carrell', 'certain', 'chance', 'respecting', 'hollander', 'homework', 'character', 'thus', 'explorations', 'narrative', 'kidnapped', 'pure', 'gain', 'arts', 'however', 'sequels', 'follows', 'chemistry', 'towards', 'horrified', 'everybody', 'particular', 'boogeyman', 'begin', 'case', 'jessica', 'puzzle', 'imdb', 'americans', 'cutouts', 'quick', 'artistic', 'unnecessary', 'marred', 'apart', 'difference', 'dumbest', 'high', 'smart', 'hosting', 'kirk', 'without', 'involves', 'jean', 'incendiary', 'adams', 'generates', 'shortlist', 'superb', 'aye', 'delivering', 'errors', 'wondered', 'start', 'embarrassing', 'sex', 'mistakes', 'dialogue', 'brainsucking', 'fantastic', 'understatement', 'nurse', 'south', 'central', 'became', 'bellucci', 'uniqueness', 'putting', 'brat', 'villain', 'always', 'discovering', 'cailles', 'long', 'q', 'coal', 'need', 'lots', 'life', 'wb', 'jet', 'exaggerating', 'fumbling', 'depressing', 'quicker', 'visual', 'lasting', 'track', 'views', 'estevez', 'integration', 'calls', 'studio', 'rare', 'primal', 'done', 'impressed', 'implausible', 'silent', 'unfaithful', 'wrote', 'egotism', 'running', 'lived', 'fit', 'memorized', 'open', 'living', 'production', 'considers', 'jutland', 'anita', 'native', 'ago', 'sinister', 'sister', 'quiet', 'flowed', 'unpredictable', 'messages', 'cheerless', 'dramatic', 'faultless', 'shooting', 'lines', 'credits', 'written', 'fulfilling', 'starlet', 'clich', 'leading', 'beware', 'potted', 'characters', 'version', 'paolo', 'ways', 'existential', 'member', 'director', 'enjoyable', 'vandiver', 'chimp', 'landscapes', 'knightley', 'aspect', 'leave', 'uneasy', 'write', 'hearts', 'pile', 'retarded', 'woven', 'telephone', 'angry', 'hernandez', 'poorly', 'raging', 'masculinity', 'fully', 'belmondo', 'solid', 'amateurish', 'genre', 'physical', 'lighting', 'anthony', 'owned', 'pearls', 'celebrity', 'wayne', 'endlessly', 'compromise', 'user', 'energy', 'plmer', 'storyline', 'fire', 'relation', 'holes', 'configuration', 'study', 'middle', 'rita', 'starts', 'sounded', 'dickens', 'moved', 'camerawork', 'avoided', 'personalities', 'trek', 'opened', 'mindblowing', 'business', 'strives', 'comedic', 'minor', 'ineptly', 'puppets', 'cardellini', 'beyond', 'heads', 'songs', 'provoking', 'monster', 'recommend', 'ackerman', 'juano', 'learn', 'rather', 'critical', 'frightening', 'meant', 'watson', 'ussr', 'rubin', 'abysmal', 'proud', 'jason', 'awkwardly', 'tired', 'usual', 'storytelling', 'lyrics', 'control', 'waitress', 'letting', 'locations', 'judo', 'makers', 'balls', 'phony', 'treasure', 'street', 'decidely', 'establish', 'leaves', 'small', 'taste', 'dream', 'gone', 'presents', 'nervous', 'relate', 'pleased', 'women', 'bordered', 'x', 'surprising', 'guy', 'bipolarity', 'practical', 'horrendously', 'making', 'emilio', 'melville', 'weariness', 'sucked', 'n', 'series', 'shell', 'keeps', 'jennifer', 'latin', 'trysts', 'camp', 'think', 'movie', 'cg', 'overcome', 'frances', 'another', 'local', 'angles', 'riveted', 'truck', 'emerge', 'become', 'beginning', 'kathy', 'crocdodile', 'exploit', 'bothersome', 'mirrormask', 'joe', 'tiny', 'known', 'insipid', 'step', 'christopher', 'reactions', 'understated', 'hands', 'minutes', 'obliged', 'jonah', 'received', 'treachery', 'human', 'thumper', 'bakery', 'meld', 'admins', 'addition', 'horror', 'satanic', 'efforts', 'barking', 'assistant', 'three', 'flawed', 'cheek', 'motivations', 'sounds', 'appalling', 'transcend', 'restrained', 'lance', 'produced', 'loneliness', 'assante', 'unmoving', 'watkins', 'falwell', 'conceptually', 'require', 'underlying', 'highest', 'fest', 'taking', 'hang', 'union', 'ridiculous', 'boyfriend', 'move', 'confusing', 'dark', 'artless', 'occasionally', 'true', 'ultra', 'incredible', 'range', 'creativity', 'roth', 'chick', 'candace', 'stage', 'ann', 'whine', 'watchable', 'deep', 'feel', 'grace', 'marbles', 'slurs', 'content', 'earlier', 'northern', 'eyes', 'buffet', 'cutie', 'underappreciated', 'tanks', 'period', 'add', 'aesthetically', 'woa', 'managed', 'comes', 'inspiration', 'zombie', 'forever', 'bitchy', 'bergen', 'creature', 'allowing', 'knew', 'length', 'swamp', 'takes', 'garbage', 'brother', 'enjoy', 'emperor', 'seamless', 'revenge', 'dose', 'undoubtedly', 'losing', 'composition', 'question', 'turned', 'hilarious', 'choked', 'sublimely', 'use', 'gripping', 'george', 'engaging', 'chosen', 'mind', 'colours', 'course', 'choice', 'dialog', 'energetic', 'garbo', 'rendering', 'obviously', 'sacrifice', 'lovely', 'directorial', 'scripting', 'death', 'male', 'kid', 'powerful', 'happy', 'bombardments', 'disbelief', 'big', 'surroundings', 'starring', 'ed', 'shakespear', 'cox', 'guilt', 'collect', 'back', 'world', 'poetry', 'talk', 'bag', 'provided', 'shelf', 'pray', 'wish', 'debut', 'problem', 'perplexing', 'casting', 'wind', 'phrase', 'decay', 'one', 'sisters', 'guess', 'mercy', 'hell', 'nationalities', 'pitiful', 'gets', 'box', 'pans', 'commentary', 'females', 'argued', 'top', 'facial', 'seem', 'working', 'spoiler', 'store', 'evidently', 'ridiculousness', 'yes', 'ive', 'flakes', 'feisty', 'confidence', 'times', 'charisma', 'lead', 'angel', 'whoever', 'artiness', 'mainly', 'game', 'babysitting', 'obsessed', 'comment', 'theme', 'fleshed', 'florida', 'whole', 'fails', 'shakespears', 'cartoons', 'hay', 'president', 'memories', 'tale', 'laughable', 'edition', 'overdue', 'type', 'atmosphere', 'noir', 'tender', 'stockings', 'lilli', 'quinn', 'underlines', 'shot', 'scene', 'characterisation', 'imaginable', 'adrift', 'family', 'compelling', 'state', 'actress', 'could', 'punish', 'contract', 'improved', 'apt', 'allison', 'stinker', 'emoting', 'smiling', 'baxendale', 'relationship', 'mean', 'tedium', 'rendition', 'desperately', 'soon', 'jokes', 'earth', 'handle', 'impulse', 'hummh', 'embassy', 'colored', 'escapism', 'proudly', 'correct', 'disgusting', 'ends', 'stay', 'hour', 'forces', 'littered', 'achievement', 'often', 'survivors', 'wholesome', 'alert', 'stereotypes', 'used', 'disgrace', 'ebola', 'science', 'bertolucci', 'vampire', 'green', 'acting', 'bring', 'lassie', 'showed', 'turns', 'treat', 'youtube', 'row', 'cheap', 'graphics', 'musician', 'screamy', 'weight', 'otherwise', 'psychological', 'built', 'dysfunction', 'maker', 'reasonable', 'forgettable', 'warning', 'politically', 'thick', 'designer', 'consolations', 'gere', 'sort', 'clear', 'hip', 'b', 'akasha', 'behold', 'accused', 'muddled', 'complete', 'wall', 'convoluted', 'shame', 'matthews', 'motion', 'animated', 'awards', 'uhura', 'contained', 'babbling', 'raver', 'peaking', 'storm', 'dee', 'pretext', 'not', 'struggle', 'cowardice', 'speak', 'cutest', 'impact', 'favorite', 'level', 'astonishingly', 'brevity', 'voyage', 'places', 'superficial', 'candle', 'sydney', 'accurately', 'told', 'humanity', 'night', 'anguish', 'due', 'parts', 'corny', 'qu', 'intentions', 'volcano', 'washing', 'masterpiece', 'drag', 'shed', 'twist', 'unfortunate', 'art', 'emptiness', 'stereotypically', 'heroism', 'talent', 'designed', 'accurate', 'cliff', 'boiling', 'superlative', 'finest', 'events', 'imitation', 'iffy', 'faster', 'late', 'handles', 'moving', 'vomited', 'warmth', 'borders', 'connections', 'sad', 'questioning', 'unsatisfactory', 'twirling', 'childhood', 'tightly', 'instruments', 'ball', 'glad', 'unrestrained', 'treatments', 'negulesco', 'veteran', 'roosevelt', 'narration', 'rocks', 'fans', 'solidifying', 'sweet', 'nicola', 'involved', 'former', 'whiny', 'government', 'system', 'admitted', 'supernatural', 'inexplicable', 'twists', 'unfunny', 'horrid', 'evaluate', 'fanciful', 'explosion', 'miserable', 'evinced', 'sundays', 'affected', 'either', 'seamlessly', 'spock', 'turkey', 'girls', 'triumphed', 'exchange', 'violin', 'sick', 'sven', 'looks', 'funny', 'g', 'ending', 'handled', 'morons', 'extraordinary', 'istagey', 'based', 'lestat', 'heroine', 'effective', 'schilling', 'parents', 'smack', 'faux', 'understood', 'expansive', 'whatsoever', 'um', 'undertone', 'frontier', 'example', 'wife', 'savor', 'reviewers', 'portraying', 'accessible', 'score', 'absolutely', 'occurs', 'outlandish', 'delights', 'meagre', 'vivid', 'chase', 'honestly', 'hankies', 'excuse', 'monolog', 'today', 'second', 'agree', 'taylor', 'football', 'intelligence', 'fat', 'cult', 'sentiment', 'ass', 'significant', 'quality', 'thinking', 'seen', 'enjoyed', 'commercial', 'expected', 'tv', 'offer', 'babie', 'love', 'older', 'stagy', 'conclusion', 'called', 'interested', 'tears', 'meredith', 'california', 'works', 'numbers', 'court', 'lid', 'martin', 'wrong', 'everyone', 'crime', 'biggest', 'longer', 'iso', 'jessice', 'scared', 'cheesy', 'run', 'delivered', 'escalating', 'time', 'southern', 'direct', 'anyone', 'suffered', 'overacting', 'meaning', 'riz', 'try', 'waster', 'unpleasant', 'terms', 'complex', 'garfield', 'faithful', 'meteorite', 'connor', 'tries', 'let', 'including', 'impression', 'cute', 'masterpieces', 'gradually', 'evil', 'gem', 'underbite', 'age', 'teeth', 'hill', 'ray', 'atrocious', 'pictures', 'fodder', 'terror', 'jealousy', 'kristoffersen', 'ups', 'paul', 'surely', 'diving', 'poised', 'disliked', 'decent', 'oriented', 'particularly', 'legal', 'actually', 'diaper', 'indoor', 'head', 'offend', 'secondly', 'wilkinson', 'nothing', 'soul', 'note', 'hellish', 'spiffy', 'attempted', 'reminded', 'considering', 'okay', 'depicts', 'brian', 'later', 'aired', 'repeated', 'went', 'fame', 'nevsky', 'describes', 'lazy', 'portrayals', 'loads', 'casted', 'conrad', 'rice', 'frankly', 'plot', 'missed', 'captured', 'leaving', 'forgetting', 'horse', 'enhanced', 'nerves', 'amusing', 'tsunami', 'carries', 'schoolers', 'almost', 'bear', 'gifted', 'thrillers', 'really', 'howell', 'female', 'coaster', 'tonight', 'features', 'bother', 'view', 'sleep', 'plays', 'galley', 'european', 'lacks', 'gake', 'silly', 'breeders', 'roeg', 'bohemian', 'commented', 'journey', 'wrap', 'shut', 'looked', 'barely', 'helping', 'focus', 'hated', 'helms', 'refreshing', 'atrocity', 'iq', 'touches', 'unrealistic', 'stinks', 'vessel', 'darren', 'performance', 'distract', 'simmering', 'indulgent', 'important', 'worthwhile', 'imperial', 'desperation', 'draft', 'logic', 'oscar', 'theatre', 'upa', 'punches', 'fact', 'cable', 'betty', 'never', 'estate', 'nut', 'tomorrow', 'awful', 'crowe', 'outlets', 'uptight', 'tony', 'liked', 'era', 'easy', 'major', 'pleaser', 'angeles', 'alike', 'greatness', 'story', 'many', 'broke', 'needed', 'mollusk', 'donlevy', 'alongside', 'little', 'versus', 'sets', 'appearance', 'bad', 'natural', 'stable', 'directors', 'timing', 'thanks', 'exceptional', 'say', 'club', 'poor', 'kind', 'emily', 'underrated', 'nice', 'believed', 'amust', 'heartwarming', 'groove', 'baby', 'players', 'wanted', 'borrowed', 'masculine', 'design', 'artist', 'poignant', 'lane', 'material', 'social', 'tremendously', 'lieutenant', 'balance', 'acted', 'humans', 'producers', 'powerhouse', 'supposedly', 'man', 'horrendous', 'lower', 'ms', 'delivers', 'son', 'riot', 'achille', 'melodrama', 'imagination', 'wow', 'blare', 'waylaid', 'sandra', 'photograph', 'unforgettable', 'curtain', 'harris', 'haggis', 'passed', 'yet', 'farce', 'accolades', 'appealing', 'train', 'explanation', 'comforting', 'miss', 'entirely', 'details', 'plus', 'lot', 'call', 'barcelona', 'mountain', 'endearing', 'wont', 'neighbour', 'pencil', 'animals', 'thoughts', 'hayao', 'canada', 'outward', 'goalies', 'award', 'custer', 'painted', 'steve', 'racial', 'insane', 'showcasing', 'titta', 'actions', 'crew', 'violence', 'wooden', 'bell', 'cinematography', 'jobs', 'prompted', 'plants', 'composed', 'expression', 'cotton', 'latched', 'impressive', 'english', 'taken', 'supporting', 'miner', 'worry', 'create', 'dollars', 'master', 'face', 'possibly', 'interesting', 'novella', 'th', 'slackers', 'predict', 'inspiring', 'swords', 'celebration', 'glasses', 'rejection', 'uncalled', 'served', 'horrible', 'spoilers', 'charm', 'old', 'considered', 'convey', 'worst', 'concentrate', 'verbatim', 'far', 'vivian', 'go', 'friends', 'give', 'grates', 'show', 'suggest', 'resume', 'bible', 'unemployed', 'bonus', 'indication', 'basic', 'march', 'unlockable', 'maybe', 'famed', 'ended', 'greatest', 'hard', 'remember', 'flaw', 'sobering', 'industry', 'trip', 'plain', 'shined', 'stephen', 'malta', 'stratus', 'gaudi', 'notable', 'developments', 'austen', 'lifetime', 'humor', 'edward', 'experience', 'mother', 'everything', 'portrayed', 'slow', 'barren', 'space', 'freshness', 'includes', 'blandly', 'drifting', 'extant', 'cars', 'trying', 'young', 'daughter', 'people', 'aerial', 'purity', 'sloppy', 'ruthless', 'latest', 'convince', 'sells', 'forth', 'agreed', 'kevin', 'guards', 'bat', 'gadget', 'macbeth', 'lilt', 'transfers', 'sorry', 'sucks', 'contributory', 'abandoned', 'owns', 'remaining', 'release', 'average', 'noble', 'side', 'complexity', 'planned', 'gay', 'care', 'rubbish', 'spew', 'released', 'rpger', 'especially', 'frost', 'underneath', 'ceases', 'product', 'routine', 'disappointing', 'super', 'says', 'taelons', 'happened', 'completely', 'positive', 'monotonous', 'hbo', 'de', 'tell', 'indie', 'scary', 'manna', 'watched', 'glance', 'ask', 'naked', 'sequel', 'koteas', 'u', 'dedication', 'ought', 'cords', 'condescends', 'see', 'simplifying', 'mouse', 'idiot', 'girl', 'edge', 'worthy', 'elegant', 'supposed', 'olivia', 'ishioka', 'stranger', 'sing', 'carol', 'murdering', 'attempts', 'century', 'student', 'meanings', 'dealt', 'dreary', 'picture', 'senior', 'feature', 'scale', 'inexperience', 'shattered', 'no', 'hollow', 'foxx', 'notch', 'miyazaki', 'loewenhielm', 'hope', 'appreciate', 'jack', 'entertaining', 'string', 'monica', 'chodorov', 'aged', 'comments', 'reverse', 'excessively', 'unbelievable', 'ugly', 'stayed', 'reviewer', 'eccleston', 'caught', 'professor', 'avoid', 'general', 'holding', 'filmmaker', 'fall', 'anyway', 'trash', 'straw', 'paint', 'cool', 'author', 'debated', 'flick', 'angela', 'semi', 'rent', 'virus', 'brilliance', 'wonderful', 'five', 'bailey', 'recover', 'psychotic', 'heroes', 'im', 'enter', 'bore', 'else', 'surrounding', 'smoothly', 'critic', 'spy', 'actresses', 'applause', 'dumb', 'mchattie', 'mediocre', 'belly', 'pull', 'cheesiness', 'screened', 'tardis', 'consequences', 'moments', 'interest', 'scare', 'angus', 'embarrassed', 'omit', 'viewer', 'feelings', 'short', 'ugliest', 'aversion', 'non', 'duet', 'rise', 'television', 'nimoy', 'day', 'friendship', 'bored', 'elsewhere', 'unmitigated', 'trailer', 'past', 'sensibility', 'beautiful', 'vitally', 'vibe', 'lack', 'plane', 'process', 'keith', 'drift', 'reconciliation', 'repeating', 'thunderbirds', 'reality', 'checking', 'recent', 'review', 'tract', 'would', 'viewing', 'realised', 'enough', 'girolamo', 'italian', 'half', 'thrown', 'incomprehensible', 'dangerous', 'backed', 'stars', 'fun', 'piece', 'witty', 'whenever', 'nature', 'going', 'sinking', 'celluloid', 'speaking', 'leni', 'gloriously', 'surprised', 'unethical', 'pace', 'julian', 'home', 'insult', 'drawings', 'related', 'relationships', 'valentine', 'card', 'tacky', 'anatomist', 'unless', 'cheekbones', 'kids', 'excuses', 'superbly', 'prepared', 'water', 'effort', 'bullock', 'spent', 'proceedings', 'hero', 'direction', 'relief', 'creative', 'continuously', 'empty', 'gotta', 'fx', 'theatres', 'moves', 'teacher', 'result', 'contains', 'crayons', 'anniversary', 'sean', 'upper', 'sitcoms', 'structure', 'definitely', 'prelude', 'like', 'given', 'forget', 'makes', 'stanwyck', 'paid', 'improvement', 'temperaments', 'ratings', 'tensions', 'job', 'scientist', 'unbearably', 'helps', 'success', 'also', 'part', 'around', 'scamp', 'stealing', 'f', 'hours', 'style', 'set', 'comical', 'book', 'remarkable', 'premise', 'turn', 'lives', 'along', 'austere', 'raw', 'accents', 'hear', 'trumpeter', 'fine', 'brutal', 'contributing', 'feet', 'cruel', 'joyce', 'children', 'overwrought', 'appropriate', 'convention', 'pair', 'snider', 'sack', 'inside', 'damn', 'fausa', 'backdrop', 'intoning', 'ability', 'episode', 'tolerate', 'ford', 'excruciatingly', 'trilogy', 'cost', 'smile', 'depicted', 'barney', 'may', 'season', 'constant', 'entire', 'thrilled', 'website', 'writer', 'self', 'minute', 'genuine', 'realized', 'fulci', 'worked', 'intelligent', 'ham', 'marriage', 'disney', 'watch', 'doctor', 'unremarkable', 'worth', 'brain', 'warts', 'twice', 'legendary', 'damian', 'trooper', 'least', 'camera', 'attempt', 'mostly', 'stick', 'humour', 'american', 'mad', 'scream', 'boost', 'naughty', 'ventura', 'eating', 'excellent', 'duris', 'london', 'savalas', 'quaid', 'circumstances', 'carry', 'plenty', 'attempting', 'terrible', 'hoffman', 'heaven', 'origins', 'deserving', 'ballet', 'fashioned', 'broad', 'evokes', 'seems', 'person', 'thing', 'interplay', 'paced', 'get', 'predictably', 'repeats', 'interim', 'relying', 'succeeded', 'behind', 'rivalry', 'nostalgia', 'philippa', 'passion', 'toons', 'roles', 'obvious', 'murky', 'hair', 'biographical', 'identifies', 'pm', 'killer', 'frustration', 'regardless', 'muppets', 'nor', 'ground', 'menace', 'knows', 'lust', 'cardboard', 'awarded', 'made', 'indictment', 'thought', 'noteworthy', 'class', 'cruise', 'humorous', 'take', 'highlights', 'funniest', 'patriotism', 'finally', 'improvisation', 'drooling', 'inappropriate', 'morgan', 'howdy', 'dvd', 'jerry', 'loyalty', 'unrecommended', 'although', 'truly', 'came', 'enjoyment', 'occupied', 'fantasy', 'gorman', 'dogs', 'mesmerising', 'rickman', 'tickets', 'screenwriter', 'produce', 'teenagers', 'lino', 'band', 'garage', 'corn', 'still', 'academy', 'subject', 'air', 'following', 'junk', 'linda', 'coastal', 'whatever', 'la', 'grimes', 'mexican', 'subtitles', 'marine', 'grade', 'grim', 'succeeds', 'enterprise', 'different', 'walk', 'timers', 'confirm', 'struck', 'someone', 'recommended', 'cliche', 'ever', 'brief', 'song', 'trinity', 'reenactments', 'excerpts', 'admiration', 'fascinated', 'snow', 'modest', 'speed', 'shatner', 'eighth', 'entertained', 'extraneous', 'loved', 'theater', 'overs', 'masterful', 'dribble', 'end', 'sense', 'receive', 'somewhat', 'males', 'gabriel', 'annoying', 'charlie', 'thorsen', 'tops', 'perfectly', 'tension', 'younger', 'heche', 'insomniacs', 'lee', 'discomfort', 'help', 'light', 'honest', 'suggests', 'promote', 'writers', 'sorrentino', 'containing', 'discovery', 'moral', 'aurv', 'bland', 'relatively', 'retreat', 'deserves', 'repertory', 'regret', 'decipher', 'effect', 'effects', 'forgot', 'clearly', 'strident', 'lucio', 'cheaply', 'jay', 'lion', 'park', 'gerardo', 'paper', 'radiant', 'lets', 'role', 'array', 'limited', 'captain', 'confuses', 'blake', 'wave', 'fingernails', 'essence', 'task', 'easily', 'ue', 'hide', 'palance', 'pixar', 'actors', 'fox', 'except', 'build', 'deadly', 'classic', 'prone', 'generally', 'classical', 'buildings', 'slavic', 'strokes', 'right', 'constructed', 'touching', 'syrupy', 'several', 'derivative', 'drawn', 'spoil', 'bop', 'unpredictability', 'kieslowski', 'word', 'peter', 'goremeister', 'low', 'duper', 'post', 'though', 'grainy', 'cgi', 'dustin', 'james', 'gently', 'breaking', 'pacing', 'perhaps', 'remake', 'expecting', 'executed', 'films', 'latifa', 'bully', 'setting', 'ho', 'dont', 'audience', 'exciting', 'father', 'attention', 'real', 'intense', 'worse', 'cutting', 'tuneful', 'universal', 'plug', 'writing', 'trashy', 'loose', 'translate', 'pretty', 'found', 'zombiez', 'drive', 'aspects', 'acclaimed', 'weaving', 'howe', 'know', 'skilled', 'black', 'goes', 'finds', 'inconsistencies', 'translating', 'splendid', 'stocking', 'hayworth', 'seat', 'afraid', 'amazingly', 'kill', 'yelps', 'round', 'emotionally', 'dislike', 'volatile', 'pi', 'hence', 'rest', 'trap', 'william', 'sum', 'dull', 'racism', 'marion', 'unrecognizable', 'africa', 'reporter', 'already', 'forwarded', 'cinema', 'power', 'lost', 'grew', 'summary', 'crashed', 'keira', 'unmatched', 'fascinating', 'negative', 'detailing', 'pathetic', 'microsoft', 'next', 'monumental', 'movements', 'applauded', 'audio', 'concert', 'mansonites', 'photography', 'rating', 'realistic', 'hackneyed', 'better', 'occur', 'robotic', 'played', 'kris', 'intrigued', 'sidelined', 'blah', 'uses', 'nuns', 'intangibles', 'beautifully', 'explains', 'rumbles', 'teen', 'olde', 'buy', 'boss', 'final', 'years', 'articulated', 'lie', 'follow', 'comprehensible', 'talented', 'yun', 'bought', 'offers', 'extremely', 'massive', 'surprises', 'mickey', 'badly', 'culture', 'alexander', 'wonderfully', 'ursula', 'sarcophage', 'catchy', 'lady', 'imaginative', 'propaganda', 'ponyo', 'luv', 'flashbacks', 'pulls', 'amazed', 'ranks', 'becomes', 'fields', 'bold', 'torture', 'students', 'sci', 'distorted', 'armageddon', 'coppola', 'experiences', 'bonding', 'redeeming', 'taxidermists', 'shepard', 'ghibili', 'primary', 'watching', 'progresses', 'great', 'eloquently', 'ryans', 'boobs', 'builders', 'bendingly', 'incorrectness', 'technically', 'conflict', 'boasts', 'columbo', 'debits', 'subverting', 'sibling', 'lame', 'girlfriend', 'mighty', 'costumes', 'balanced', 'indescribably', 'regrettably', 'music', 'fifties', 'political', 'wonder', 'laughs', 'hold', 'save', 'defined', 'values', 'unbearable', 'los', 'kitchy', 'schultz', 'idiotic', 'shirley', 'wartime', 'good', 'hype', 'scripts', 'player', 'awesome', 'killings', 'school', 'waiting', 'pulling', 'dominated', 'blew', 'editing', 'machine', 'sheer', 'something', 'form', 'sequences', 'cinematic', 'telly', 'mclaglen', 'shameful', 'room', 'yeah', 'joy', 'hand', 'relaxing', 'choices', 'deeply', 'money', 'elaborately', 'suffering', 'wasting', 'took', 'spoiled', 'revere', 'format', 'skip', 'expert', 'redeemed', 'random', 'meanders', 'started', 'suck', 'afternoon', 'smells', 'vehicles', 'nearly', 'flat', 'explain', 'brigand', 'pointillistic', 'house', 'star', 'bechard', 'buffalo', 'picked', 'reading', 'switched', 'mark', 'fish', 'touch', 'childrens', 'roller', 'wong', 'believable', 'shenanigans', 'provokes', 'chow', 'realize', 'mature', 'uplifting', 'irons', 'well', 'judith', 'entrance', 'net', 'trumbull', 'youthful', 'dry', 'sand', 'play', 'established', 'reasons', 'stories', 'rpg', 'joins', 'said', 'suited', 'dropped', 'weak', 'downs', 'slightest', 'bright', 'factory', 'national', 'partaking', 'labute', 'clever', 'disappointed', 'amaze', 'couple', 'punishment', 'generic', 'results', 'suspension', 'special', 'perfect', 'episodes', 'gosh', 'traffic', 'develop', 'worthless', 'seuss', 'aailiyah', 'words', 'savant', 'gotten', 'comfortable', 'stuff', 'latter', 'channel', 'hot', 'tying', 'dancing', 'stuart', 'sour', 'sink', 'costs', 'crisp', 'romantic', 'ladies', 'literally', 'facing', 'nuts', 'hairsplitting', 'centers', 'jaclyn', 'overly', 'original', 'delightful', 'sure', 'deal', 'controversy', 'seriously', 'integral', 'wild', 'exactly', 'tranquillity', 'crap', 'force', 'john', 'flicks', 'murder', 'prejudice', 'change', 'blatant', 'pitch', 'issues', 'knocked', 'anything', 'appears', 'versatile', 'professionals', 'credit', 'ticker', 'charming', 'properly', 'got', 'cameo', 'full', 'soap', 'oy', 'describe', 'jamie', 'collective', 'odd', 'crowd', 'washed', 'abroad', 'stand', 'sites', 'rocked', 'trond', 'hockey', 'standout', 'charles', 'christmas', 'ironside', 'dreams', 'owls', 'sensitivities', 'thug', 'slimy', 'emotion', 'defensemen', 'certainly', 'sat', 'cheerfull', 'cases', 'ben', 'matrix', 'bates', 'individual', 'doubt', 'probably', 'aimless', 'disaster', 'reflected', 'falls', 'vocal', 'unintentionally', 'justice', 'pack', 'di', 'typical', 'ten', 'distant', 'script', 'added', 'favourite', 'bennett', 'gives', 'filmiing', 'work', 'exceptionally', 'cole', 'consistent', 'us', 'adaptation', 'site', 'number', 'bob', 'inventive', 'punched', 'talents', 'continuity', 'angle', 'scripted', 'interpretations', 'crackles', 'tom', 'blush', 'team', 'cat', 'hitchcock', 'loves', 'higher', 'new', 'dead', 'pleasing', 'subversive', 'rolls', 'gas', 'act', 'filmography', 'fisted', 'crash', 'serious', 'lewis', 'equivalent', 'every', 'pg', 'close', 'drama', 'hugo', 'freedom', 'popcorn', 'dodge', 'mess', 'dozen', 'schizophrenic', 'problems', 'cannot', 'wouldnt', 'patent', 'scot', 'title', 'pyromaniac', 'weaker', 'themes', 'unfolds', 'layers', 'threshold', 'element', 'renowned', 'gallon', 'stewart', 'body', 'emotions', 'secondary', 'ireland', 'want', 'jones', 'dignity', 'sake', 'instant', 'desert', 'heard', 'shocking', 'directed', 'dimensional', 'bonuses', 'excellently', 'largely', 'selections', 'reviews', 'poet', 'nevertheless', 'best', 'fill', 'judge', 'sympathetic', 'climax', 'pay', 'nine', 'highly', 'ones', 'gore', 'wanting', 'leap', 'ages', 'voice', 'scrimm', 'crafted', 'terminology', 'exemplars', 'ferry', 'netflix', 'depends', 'french', 'practically', 'ordeal', 'cross', 'lucy', 'surface', 'errol', 'sabotages', 'initially', 'games', 'put', 'budget', 'wedding', 'subplots', 'fairly', 'truth', 'wise', 'empowerment', 'america', 'advise', 'screenplay', 'idea', 'francis', 'crazy', 'cover', 'ortolani', 'brooding', 'menacing', 'burton', 'disappointment', 'blood', 'dads', 'pillow', 'sublime', 'likes', 'return', 'removing', 'mini', 'bunch', 'laugh', 'within', 'readers', 'sub', 'sam', 'matter', 'delete', 'impossible', 'looking', 'nonetheless', 'verbal', 'sincere', 'none', 'keep', 'parker', 'qualities', 'cuts', 'researched', 'virtue', 'angelina', 'famous', 'attractive', 'comedy', 'overt', 'fi', 'jimmy', 'vulcan', 'remotely', 'forgotten', 'began', 'assaulted', 'free', 'sit', 'momentum', 'junkyard', 'find', 'utterly', 'theatrical', 'heels', 'steamboat', 'rescue', 'list', 'jerky', 'sea', 'heist', 'conception', 'seperate', 'bond', 'continue', 'thriller', 'despised', 'might', 'fishnet', 'fear', 'even', 'strong', 'hate', 'phenomenal', 'fair', 'hilt', 'brilliant', 'things', 'dracula', 'medical', 'last', 'interview', 'fabulous', 'puppet', 'hes', 'murdered', 'disparate', 'core', 'amount', 'trouble', 'expect', 'laselva', 'future', 'v', 'wih', 'happen', 'thousand', 'soldiers', 'church', 'chills', 'lousy', 'lovable', 'memorable', 'exteriors', 'slightly', 'involving', 'quite', 'e', 'warn', 'changes', 'hollywood', 'elderly', 'continually', 'created', 'whites', 'lugosi', 'rough', 'giving', 'first', 'solving', 'action', 'sometimes', 'michael', 'spot', 'closed', 'seemed', 'movies', 'elias', 'etc', 'shows', 'squibs', 'falling', 'since', 'bec', 'billy', 'fresh', 'antithesis', 'halfway', 'gibberish', 'merit', 'soundtrack', 'sharing', 'wily', 'away', 'filmed', 'sits', 'flaming', 'sculpture', 'less', 'sole', 'gonna', 'tremendous', 'idyllic', 'baaaaaad', 'cold', 'much', 'freeman', 'flynn', 'strange', 'unaccompanied', 'coherent', 'deserved', 'failed', 'together', 'repair', 'japanese', 'personally', 'peculiarity', 'painfully', 'sphere', 'dance', 'singing', 'misplace', 'tolerable', 'phantasm', 'fellowes', 'traditional', 'stupid', 'perabo', 'falsely', 'rate', 'allow', 'moment', 'clothes', 'sound', 'line', 'widmark', 'chilly', 'existent', 'underacting', 'courtroom', 'distressed', 'reason', 'sharply', 'geek', 'lacked', 'come', 'delight', 'early', 'adorable', 'preservation', 'hanks', 'huston', 'computer', 'situations', 'brilliantly', 'fast', 'make', 'concerns', 'irritating', 'fort', 'wasted', 'guests', 'single', 'utter', 'robert', 'loosely', 'magnificent', 'lord', 'location', 'shots', 'stylized', 'r', 'brings', 'presence', 'film', 'originality', 'abstruse', 'screen', 'front', 'precisely', 'sample', 'way', 'tons', 'armand', 'footage', 'pseudo', 'development', 'affleck', 'pedestal', 'year', 'doomed', 'linear', 'modern', 'thomerson', 'company', 'townsend', 'senses', 'lange', 'tortured', 'hypocrisy', 'directing', 'limitations', 'jim', 'credible', 'outside', 'bit', 'normally', 'deliver', 'faces', 'dwight'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "again dict  {'zombiez': 6.922918004572872, 'zillion': 6.922918004572872, 'yun': 6.922918004572872, 'youtube': 6.922918004572872, 'youthful': 6.922918004572872, 'younger': 6.922918004572872, 'yelps': 6.922918004572872, 'yawn': 6.922918004572872, 'yardley': 6.922918004572872, 'wrote': 6.922918004572872, 'writers': 6.922918004572872, 'wrap': 6.922918004572872, 'wow': 6.922918004572872, 'woven': 6.922918004572872, 'wouldnt': 6.922918004572872, 'worthwhile': 6.922918004572872, 'worthless': 6.922918004572872, 'worry': 6.922918004572872, 'worked': 6.922918004572872, 'wont': 6.922918004572872, 'wong': 6.922918004572872, 'wondered': 6.922918004572872, 'woa': 6.922918004572872, 'witticisms': 6.922918004572872, 'within': 6.922918004572872, 'wily': 6.922918004572872, 'willie': 6.922918004572872, 'william': 6.922918004572872, 'wild': 6.922918004572872, 'wih': 6.922918004572872, 'wife': 6.922918004572872, 'widmark': 6.922918004572872, 'wide': 6.922918004572872, 'whoever': 6.922918004572872, 'whites': 6.922918004572872, 'whine': 6.922918004572872, 'whenever': 6.922918004572872, 'went': 6.922918004572872, 'welsh': 6.922918004572872, 'weight': 6.922918004572872, 'wedding': 6.922918004572872, 'website': 6.922918004572872, 'weaving': 6.922918004572872, 'weariness': 6.922918004572872, 'weaker': 6.922918004572872, 'wayne': 6.922918004572872, 'waylaid': 6.922918004572872, 'wave': 6.922918004572872, 'wasting': 6.922918004572872, 'waster': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "task2_features = fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'waster': 6.922918004572872,\n",
       " 'wasting': 6.922918004572872,\n",
       " 'wave': 6.922918004572872,\n",
       " 'waylaid': 6.922918004572872,\n",
       " 'wayne': 6.922918004572872,\n",
       " 'weaker': 6.922918004572872,\n",
       " 'weariness': 6.922918004572872,\n",
       " 'weaving': 6.922918004572872,\n",
       " 'website': 6.922918004572872,\n",
       " 'wedding': 6.922918004572872,\n",
       " 'weight': 6.922918004572872,\n",
       " 'welsh': 6.922918004572872,\n",
       " 'went': 6.922918004572872,\n",
       " 'whenever': 6.922918004572872,\n",
       " 'whine': 6.922918004572872,\n",
       " 'whites': 6.922918004572872,\n",
       " 'whoever': 6.922918004572872,\n",
       " 'wide': 6.922918004572872,\n",
       " 'widmark': 6.922918004572872,\n",
       " 'wife': 6.922918004572872,\n",
       " 'wih': 6.922918004572872,\n",
       " 'wild': 6.922918004572872,\n",
       " 'william': 6.922918004572872,\n",
       " 'willie': 6.922918004572872,\n",
       " 'wily': 6.922918004572872,\n",
       " 'within': 6.922918004572872,\n",
       " 'witticisms': 6.922918004572872,\n",
       " 'woa': 6.922918004572872,\n",
       " 'wondered': 6.922918004572872,\n",
       " 'wong': 6.922918004572872,\n",
       " 'wont': 6.922918004572872,\n",
       " 'worked': 6.922918004572872,\n",
       " 'worry': 6.922918004572872,\n",
       " 'worthless': 6.922918004572872,\n",
       " 'worthwhile': 6.922918004572872,\n",
       " 'wouldnt': 6.922918004572872,\n",
       " 'woven': 6.922918004572872,\n",
       " 'wow': 6.922918004572872,\n",
       " 'wrap': 6.922918004572872,\n",
       " 'writers': 6.922918004572872,\n",
       " 'wrote': 6.922918004572872,\n",
       " 'yardley': 6.922918004572872,\n",
       " 'yawn': 6.922918004572872,\n",
       " 'yelps': 6.922918004572872,\n",
       " 'younger': 6.922918004572872,\n",
       " 'youthful': 6.922918004572872,\n",
       " 'youtube': 6.922918004572872,\n",
       " 'yun': 6.922918004572872,\n",
       " 'zillion': 6.922918004572872,\n",
       " 'zombiez': 6.922918004572872}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872, 6.922918004572872])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_features.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(corpus,task2_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sort_dictionary_by_value(dictionary, t_reverse=False):\n",
    "    '''\n",
    "    This function reverse the dictionary according to the value in ascending or descending order\n",
    "    this function returns the sorted dictionary \n",
    "    >>> temp_dict = {'a':20,'b':3,'c':5,'d':8}\n",
    "    >>> sorted_dict = sort_dictionary_by_value(temp_dict,True)\n",
    "    >>> sorted_dict\n",
    "    O[1]: {'a':20,'d':8,'c':5,'b':3}\n",
    "    '''\n",
    "    # change the dictionary into list of tuples of dictionary's values and keys\n",
    "    # step1-> access dictionary's values\n",
    "    # step2-> change step1 to list\n",
    "    # step3-> access dictionary's keys\n",
    "    # step4-> change step 3 to list\n",
    "    # step5-> zip step2 and step4(values and keys lists)\n",
    "    # step6-> typecast it to list again\n",
    "    temp_list = list(zip(list(dictionary.values()),list(dictionary.keys())))\n",
    "    \n",
    "    # Now, let's sort the list\n",
    "    temp_list.sort(reverse = t_reverse)\n",
    "    \n",
    "    for i, item in enumerate(temp_list):\n",
    "        temp = list(item)\n",
    "        temp.reverse()\n",
    "        temp = tuple(temp)\n",
    "        # override the temp_list\n",
    "        temp_list[i] = temp\n",
    "        \n",
    "    # selecting top 50 features according to their idf values\n",
    "#     t_list = temp_list[0:50]\n",
    "    \n",
    "    again_dict = dict(temp_list) # returning all the features\n",
    "    \n",
    "    return again_dict# returning the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to find the maximum tf among the whole documents\n",
    "def max_term_frequency(dataset,in_word):\n",
    "    '''\n",
    "    this method returns the list of highest term-frequency of given list of words.\n",
    "    '''\n",
    "    count =0\n",
    "    max_tf = 0\n",
    "    tf = 0\n",
    "    temp_list=[]# to store tfs\n",
    "    for item in in_word:\n",
    "        for row in dataset:\n",
    "            count = 0\n",
    "            for word in row.split(' '):\n",
    "                if item== word:\n",
    "                    count+=1\n",
    "            tf = count/len(row.split(' '))\n",
    "            if max_tf < tf:\n",
    "                max_tf = tf\n",
    "        temp_list.append(max_tf)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sort_dictionary_by_value(dictionary, t_reverse=False):\n",
    "    '''\n",
    "    This function reverse the dictionary according to the value in ascending or descending order\n",
    "    this function returns the sorted dictionary \n",
    "    >>> temp_dict = {'a':20,'b':3,'c':5,'d':8}\n",
    "    >>> sorted_dict = sort_dictionary_by_value(temp_dict,True)\n",
    "    >>> sorted_dict\n",
    "    O[1]: {'a':20,'d':8,'c':5,'b':3}\n",
    "    '''\n",
    "    # change the dictionary into list of tuples of dictionary's values and keys\n",
    "    # step1-> access dictionary's values\n",
    "    # step2-> change step1 to list\n",
    "    # step3-> access dictionary's keys\n",
    "    # step4-> change step 3 to list\n",
    "    # step5-> zip step2 and step4(values and keys lists)\n",
    "    # step6-> typecast it to list again\n",
    "    temp_list = list(zip(list(dictionary.values()),list(dictionary.keys())))\n",
    "    \n",
    "    # Now, let's sort the list\n",
    "    temp_list.sort(reverse = t_reverse)\n",
    "    \n",
    "    for i, item in enumerate(temp_list):\n",
    "        temp = list(item)\n",
    "        temp.reverse()\n",
    "        temp = tuple(temp)\n",
    "        # override the temp_list\n",
    "        temp_list[i] = temp\n",
    "        \n",
    "    # selecting top 50 features according to their idf values\n",
    "#     t_list = temp_list[0:50]\n",
    "    \n",
    "    again_dict = dict(temp_list) # returning all the features\n",
    "    \n",
    "    return again_dict# returning the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to find the maximum tf among the whole documents\n",
    "def max_term_frequency(dataset,in_word):\n",
    "    '''\n",
    "    this method returns the list of highest term-frequency of given list of words.\n",
    "    '''\n",
    "    count =0\n",
    "    max_tf = 0\n",
    "    tf = 0\n",
    "    temp_list=[]# to store tfs\n",
    "    for item in in_word:\n",
    "        max_tf = 0\n",
    "        count = 0\n",
    "        len_of_review= 0\n",
    "        for row in dataset:\n",
    "#             len_row = len(row.split(' '))\n",
    "            # finding the no_of_review which contain perticular word\n",
    "            if item in row.split(' '):\n",
    "                len_of_review+= len(row.split(' '))\n",
    "            \n",
    "            for word in row.split(' '):\n",
    "                if item== word:\n",
    "                    count+=1\n",
    "        tf = count/len_of_review\n",
    "#             if max_tf < tf:\n",
    "#                 max_tf = tf\n",
    "        temp_list.append(tf)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create tfidf fit function\n",
    "\n",
    "# let's do some change in fit function.\n",
    "# impelement maximum feature functionality\n",
    "def fit(dataset):\n",
    "    '''\n",
    "    It returns the dictionary of feature names and their idf\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty set to store unique words\n",
    "    # in the corpus to find idf of that word\n",
    "    unique_words = set() \n",
    "    # initialize feature_idf dictionary to return the final result\n",
    "    feature_idf = dict() \n",
    "    \n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset,(list,)):\n",
    "        # first find the unique words in the dataset\n",
    "        for row in dataset:\n",
    "            for word in (row.split()):\n",
    "                unique_words.add(word)\n",
    "        \n",
    "#         print(unique_words)\n",
    "        # how many reviews are there in the dataset\n",
    "        # to find the idf value\n",
    "        no_of_reviews = len(dataset)\n",
    "        \n",
    "        \n",
    "        for word in unique_words:# for each unique word in the reivew\n",
    "                if len(word)<2:# It is found that adjective has no less than 2 words\n",
    "                    continue\n",
    "                    \n",
    "                # otherwise\n",
    "                \n",
    "                # find how many reviews containing the given 'word'\n",
    "                reviews_contain_word = containing_word(dataset,word)\n",
    "                \n",
    "                # calculate the idf value according to the formula in sklearn official documentation\n",
    "                # to overcome the problem of zero division error\n",
    "                idf_value = 1+ math.log((1+no_of_reviews)/(1+reviews_contain_word))\n",
    "                \n",
    "                # storing the value in the dictionary\n",
    "                # key: 'word'\n",
    "                # value: 'idf' of that word\n",
    "                feature_idf[word]=idf_value\n",
    "\n",
    "        # sort dictionary by_value_return total features\n",
    "        sorted_feature_idf = sort_dictionary_by_value(feature_idf, True)\n",
    "        # sort by keys\n",
    "        sorted_feature_idf = dict(sorted(sorted_feature_idf.items(),key = lambda kv:(kv[0], kv[1])))\n",
    "        #################################################################################################\n",
    "        # Now select the top 50 features according to the highest tf of perticular word\n",
    "        features_tf = dict(zip(list(sorted_feature_idf.keys()),max_term_frequency(corpus,list(sorted_feature_idf.keys()))))\n",
    "        \n",
    "#         print(features_tf)\n",
    "        #making list of dictionary in reverse(key as value, value as key)\n",
    "        temp_list = list(zip(list(features_tf.values()),list(features_tf.keys())))\n",
    "    \n",
    "        # Now, let's sort the found list\n",
    "        temp_list.sort(reverse = True)\n",
    "\n",
    "        # swap the keys to values and vice-versa of dictionary\n",
    "        for i, item in enumerate(temp_list):\n",
    "            temp = list(item)\n",
    "            temp.reverse()\n",
    "            temp = tuple(temp)\n",
    "            # override the temp_list\n",
    "            temp_list[i] = temp\n",
    "        # selecting top 50 tf\n",
    "        t_list = temp_list[0:50]\n",
    "        new_feature_dict = dict(t_list) # dictionary with sorted tfs\n",
    "                \n",
    "        print(new_feature_dict)\n",
    "        ####################################################################################################\n",
    "        \n",
    "        #################\n",
    "        #################\n",
    "        # Now, choose top 50 features and their idf's in one dictionary\n",
    "        \n",
    "        top_dictionary={}\n",
    "        for key1 in new_feature_dict.keys():\n",
    "            for items in sorted_feature_idf.items():\n",
    "                if key1 == items[0]:\n",
    "                    top_dictionary[key1] = items[1]\n",
    "        #################\n",
    "        #################\n",
    "        #Now, sort according to their keys\n",
    "        new_feature_idf = dict(sorted(top_dictionary.items(),key = lambda kv:(kv[0], kv[1])))\n",
    "        \n",
    "        return new_feature_idf # returning unique words and their idf\n",
    "    else:\n",
    "        # if the dataset is not in the list format\n",
    "        print('you need to pass list of sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shameful': 1.0, 'horrendous': 1.0, 'blew': 1.0, 'wont': 0.5, 'unrecommended': 0.5, 'treasure': 0.5, 'transfers': 0.5, 'standout': 0.5, 'politically': 0.5, 'omit': 0.5, 'nonetheless': 0.5, 'national': 0.5, 'miss': 0.5, 'extraordinary': 0.5, 'exceptionally': 0.5, 'correct': 0.5, 'confidence': 0.5, 'shakespear': 0.4, 'checking': 0.375, 'wow': 0.3333333333333333, 'unpleasant': 0.3333333333333333, 'trumbull': 0.3333333333333333, 'triumphed': 0.3333333333333333, 'ticker': 0.3333333333333333, 'sweet': 0.3333333333333333, 'sublimely': 0.3333333333333333, 'sloppy': 0.3333333333333333, 'skilled': 0.3333333333333333, 'sad': 0.3333333333333333, 'results': 0.3333333333333333, 'raw': 0.3333333333333333, 'players': 0.3333333333333333, 'owls': 0.3333333333333333, 'ms': 0.3333333333333333, 'mesmerising': 0.3333333333333333, 'meredith': 0.3333333333333333, 'limited': 0.3333333333333333, 'julian': 0.3333333333333333, 'irons': 0.3333333333333333, 'filmiing': 0.3333333333333333, 'fellowes': 0.3333333333333333, 'expansive': 0.3333333333333333, 'exciting': 0.3333333333333333, 'evidently': 0.3333333333333333, 'dropped': 0.3333333333333333, 'delete': 0.3333333333333333, 'cutting': 0.3333333333333333, 'cutie': 0.3333333333333333, 'carries': 0.3333333333333333, 'blah': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "x = fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
